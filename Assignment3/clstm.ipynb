{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 93\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', str(text).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1490, Test samples: 232\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Datasets/TrainData.csv\")\n",
    "test_df = pd.read_csv(\"Datasets/TestLabels.csv\")\n",
    "label_col = test_df.columns[-1]\n",
    "\n",
    "train_df.dropna(subset=['Text', 'Category'], inplace=True)\n",
    "test_df.dropna(subset=['Text', label_col], inplace=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Test samples: {len(test_df)}\")\n",
    "\n",
    "train_texts = train_df['Text'].tolist()\n",
    "train_labels = train_df['Category'].tolist()\n",
    "test_texts = test_df['Text'].tolist()\n",
    "test_labels = test_df[label_col].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unnamed: 29'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab=None, label2idx=None, max_len=300):\n",
    "        self.texts = [simple_tokenizer(t) for t in texts]\n",
    "        self.max_len = max_len\n",
    "\n",
    "        if vocab is None:\n",
    "            words = [word for text in self.texts for word in text]\n",
    "            word_freq = Counter(words)\n",
    "            self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "            for word in word_freq:\n",
    "                self.vocab[word] = len(self.vocab)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.texts = [self.encode(text) for text in self.texts]\n",
    "\n",
    "        if label2idx is None:\n",
    "            unique_labels = sorted(set(label for label in labels if pd.notna(label)))\n",
    "            self.label2idx = {label: i for i, label in enumerate(unique_labels)}\n",
    "        else:\n",
    "            self.label2idx = label2idx\n",
    "\n",
    "        self.labels = [self.label2idx[label] for label in labels if pd.notna(label)]\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        encoded = [self.vocab.get(tok, self.vocab['<UNK>']) for tok in tokens]\n",
    "        return encoded[:self.max_len] + [0]*(self.max_len - len(encoded))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NewsDataset(train_texts, train_labels)\n",
    "test_data = NewsDataset(test_texts, test_labels, vocab=train_data.vocab, label2idx=train_data.label2idx)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(128, 128, batch_first=True, bidirectional=True)\n",
    "        self.attn_fc = nn.Linear(256, 1)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)              # (B, T, D)\n",
    "        x = x.permute(0, 2, 1)             # (B, D, T)\n",
    "        x = self.relu(self.conv(x))       # (B, C, T)\n",
    "        x = x.permute(0, 2, 1)             # (B, T, C)\n",
    "        lstm_out, _ = self.lstm(x)        # (B, T, 2H)\n",
    "        attn_weights = torch.softmax(self.attn_fc(lstm_out), dim=1)  # (B, T, 1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)          # (B, 2H)\n",
    "        return self.fc(context)           # (B, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for texts, labels in loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in loader:\n",
    "            texts = texts.to(device)\n",
    "            outputs = model(texts)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.tolist())\n",
    "    f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return f1, cm, all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model\n",
    "model = CLSTM(len(train_data.vocab), embed_dim=100, num_classes=len(train_data.label2idx)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Micro F1 Score = 0.3017\n",
      "Epoch 2: Micro F1 Score = 0.2888\n",
      "Epoch 3: Micro F1 Score = 0.2931\n",
      "Epoch 4: Micro F1 Score = 0.3103\n",
      "Epoch 5: Micro F1 Score = 0.3017\n",
      "Epoch 6: Micro F1 Score = 0.2974\n",
      "Epoch 7: Micro F1 Score = 0.3017\n",
      "Epoch 8: Micro F1 Score = 0.2931\n",
      "Epoch 9: Micro F1 Score = 0.2629\n",
      "Epoch 10: Micro F1 Score = 0.2371\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train_model(model, train_loader, criterion, optimizer, device)\n",
    "    f1, _, _, _ = evaluate_model(model, test_loader, device)\n",
    "    print(f\"Epoch {epoch+1}: Micro F1 Score = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Micro F1 Score = 0.2371\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation and confusion matrix\n",
    "f1, cm, y_true, y_pred = evaluate_model(model, test_loader, device)\n",
    "print(f\"\\nFinal Micro F1 Score = {f1:.4f}\")\n",
    "label_names = [label for label, _ in sorted(train_data.label2idx.items(), key=lambda x: x[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dla2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
