{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from gensim.models import Word2Vec\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/kash/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "EPOCHS=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path =\"Datasets/TrainData.csv\"\n",
    "test_path = \"Datasets/TestLabels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_df = pd.read_csv(train_path)  # Change to actual file path\n",
    "train_texts = train_df[\"Text\"].astype(str).tolist()\n",
    "train_labels = train_df[\"Category\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_path)  # Change to actual file path\n",
    "test_texts = test_df[\"Text\"].astype(str).tolist()\n",
    "test_labels = test_df[\"Label - (business, tech, politics, sport, entertainment)\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokenized = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    #filtered = [word for word in tokenized]\n",
    "    filtered = [word for word in tokenized if word not in punctuation]\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [preprocess_text(text) for text in train_texts]\n",
    "test_tokenized = [preprocess_text(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "embedding_dim = word2vec_model.vector_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec_model = api.load('word2vec-google-news-300')\n",
    "word2vec_model = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "embedding_dim = word2vec_model.vector_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Convert GloVe format to Word2Vec format\n",
    "glove_input_file = 'path_to_extracted_files/glove.6B.300d.txt'  # Use 300-dimensional embeddings\n",
    "word2vec_output_file = 'glove.6B.300d.word2vec'\n",
    "\n",
    "# Convert GloVe to Word2Vec format\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "# Load converted Word2Vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "test_labels = label_encoder.transform(test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(inner_array) for inner_array in tokenized_texts]\n",
    "max_len = int(np.percentile(lengths, 98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words: 11389\n"
     ]
    }
   ],
   "source": [
    "oov_count = sum(1 for text in tokenized_texts for word in text if word not in word2vec_model)\n",
    "print(f\"Number of OOV words: {oov_count}\")\n",
    "embeddings_list = [word2vec_model[word] for word in word2vec_model.index_to_key]\n",
    "average_embedding = np.mean(embeddings_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_embedding(text,average_embedding=average_embedding, max_len=max_len):\n",
    "    embedding = np.zeros((max_len, embedding_dim))\n",
    "    \n",
    "    for i, word in enumerate(text[:max_len]):\n",
    "        if word in word2vec_model:\n",
    "            embedding[i] = word2vec_model[word]\n",
    "        else:\n",
    "            embedding[i] = average_embedding\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([text_to_embedding(text) for text in tokenized_texts])\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "X_test = np.array([text_to_embedding(text) for text in test_tokenized])\n",
    "y_test = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(X_train, y_train)\n",
    "val_dataset = TextDataset(X_val, y_val)\n",
    "test_dataset = TextDataset(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class CLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes,out_size=64,hidden_layer = 64):\n",
    "        super(CLSTM, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=out_size, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=out_size, hidden_size=out_size, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        \n",
    "        self.attention = nn.Linear(out_size * 2, 1)  \n",
    "        \n",
    "        self.fc1 = nn.Linear(out_size * 2, hidden_layer)  \n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc2 = nn.Linear(hidden_layer, num_classes)\n",
    "        self.relu  = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Change to (batch_size, embedding_dim, seq_len)\n",
    "        x = torch.relu(self.conv1(x)) \n",
    "        \n",
    "        x = x.permute(0, 2, 1)  # Shape: (batch_size, seq_len, 128)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x)  \n",
    "        \n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)  \n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)     \n",
    "        \n",
    "        x = torch.relu(self.fc1(context_vector))  \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes,out_size=64,hidden_layer = 64):\n",
    "        super(CLSTM, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=out_size, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_size)  # Batch Normalization for Conv1d output\n",
    "        self.lstm = nn.LSTM(input_size=out_size, hidden_size=out_size, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        \n",
    "        self.attention = nn.Linear(out_size * 2, 1)  \n",
    "        \n",
    "        self.fc1 = nn.Linear(out_size * 2, hidden_layer)  \n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc2 = nn.Linear(hidden_layer, num_classes)\n",
    "        self.relu  = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Change to (batch_size, embedding_dim, seq_len)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  \n",
    "        x = self.relu(x)       # Apply Batch Normalization\n",
    "        x = x.permute(0, 2, 1)  # Shape: (batch_size, seq_len, out_size)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)  # Shape: (batch_size, seq_len, out_size * 2)\n",
    "        attention_scores = self.attention(torch.tanh(lstm_out))  # Optional nonlinearity\n",
    "        #attention_scores = self.attention(lstm_out)  # Shape: (batch_size, seq_len, 1)\n",
    "        attn_weights = torch.softmax(attention_scores.squeeze(-1), dim=1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        context_vector = torch.sum(attn_weights.unsqueeze(-1) * lstm_out, dim=1)  # Shape: (batch_size, out_size * 2)\n",
    "\n",
    "        x = torch.relu(self.fc1(context_vector))  \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10):\n",
    "    model.train()\n",
    "    patience = 20  \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0  \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += y_batch.size(0)\n",
    "            correct_train += (predicted == y_batch).sum().item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        avg_val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n",
    "        #scheduler.step()\n",
    "\n",
    "        #scheduler.step(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}% | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0  # Reset patience counter\n",
    "            best_model = model.state_dict()  # Save best model\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best val loss: {best_val_loss:.4f}\")\n",
    "            #break\n",
    "    return best_model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kash/miniconda/envs/dla2/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(set(y_train))\n",
    "model = CLSTM(embedding_dim, num_classes,hidden_layer=16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4) #4\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.004, epochs=EPOCHS, steps_per_epoch=len(train_loader))\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 1.6122 | Train Acc: 18.20% | Val Loss: 1.6105 | Val Acc: 19.13%\n",
      "Epoch 2/200 | Train Loss: 1.6131 | Train Acc: 18.20% | Val Loss: 1.6098 | Val Acc: 19.13%\n",
      "Epoch 3/200 | Train Loss: 1.6118 | Train Acc: 18.20% | Val Loss: 1.6090 | Val Acc: 19.13%\n",
      "Epoch 4/200 | Train Loss: 1.6113 | Train Acc: 18.20% | Val Loss: 1.6084 | Val Acc: 19.13%\n",
      "Epoch 5/200 | Train Loss: 1.6106 | Train Acc: 18.20% | Val Loss: 1.6076 | Val Acc: 19.13%\n",
      "Epoch 6/200 | Train Loss: 1.6098 | Train Acc: 18.20% | Val Loss: 1.6065 | Val Acc: 19.13%\n",
      "Epoch 7/200 | Train Loss: 1.6090 | Train Acc: 18.20% | Val Loss: 1.6055 | Val Acc: 19.13%\n",
      "Epoch 8/200 | Train Loss: 1.6078 | Train Acc: 18.20% | Val Loss: 1.6043 | Val Acc: 19.13%\n",
      "Epoch 9/200 | Train Loss: 1.6065 | Train Acc: 19.46% | Val Loss: 1.6023 | Val Acc: 22.82%\n",
      "Epoch 10/200 | Train Loss: 1.6040 | Train Acc: 23.32% | Val Loss: 1.5997 | Val Acc: 22.82%\n",
      "Epoch 11/200 | Train Loss: 1.5993 | Train Acc: 21.73% | Val Loss: 1.5915 | Val Acc: 23.83%\n",
      "Epoch 12/200 | Train Loss: 1.5827 | Train Acc: 24.75% | Val Loss: 1.5850 | Val Acc: 24.50%\n",
      "Epoch 13/200 | Train Loss: 1.5960 | Train Acc: 22.57% | Val Loss: 1.5846 | Val Acc: 26.17%\n",
      "Epoch 14/200 | Train Loss: 1.5594 | Train Acc: 27.60% | Val Loss: 1.5613 | Val Acc: 27.18%\n",
      "Epoch 15/200 | Train Loss: 1.5401 | Train Acc: 29.19% | Val Loss: 1.5604 | Val Acc: 29.19%\n",
      "Epoch 16/200 | Train Loss: 1.5254 | Train Acc: 27.77% | Val Loss: 1.5660 | Val Acc: 25.50%\n",
      "Epoch 17/200 | Train Loss: 1.5204 | Train Acc: 28.36% | Val Loss: 1.5538 | Val Acc: 27.18%\n",
      "Epoch 18/200 | Train Loss: 1.5155 | Train Acc: 29.03% | Val Loss: 1.5455 | Val Acc: 27.52%\n",
      "Epoch 19/200 | Train Loss: 1.5092 | Train Acc: 29.70% | Val Loss: 1.5476 | Val Acc: 27.18%\n",
      "Epoch 20/200 | Train Loss: 1.5038 | Train Acc: 29.70% | Val Loss: 1.5360 | Val Acc: 30.54%\n",
      "Epoch 21/200 | Train Loss: 1.4966 | Train Acc: 28.52% | Val Loss: 1.5373 | Val Acc: 28.19%\n",
      "Epoch 22/200 | Train Loss: 1.5012 | Train Acc: 30.29% | Val Loss: 1.5497 | Val Acc: 27.85%\n",
      "Epoch 23/200 | Train Loss: 1.4990 | Train Acc: 31.38% | Val Loss: 1.5275 | Val Acc: 30.20%\n",
      "Epoch 24/200 | Train Loss: 1.5005 | Train Acc: 29.11% | Val Loss: 1.5653 | Val Acc: 28.19%\n",
      "Epoch 25/200 | Train Loss: 1.5096 | Train Acc: 30.12% | Val Loss: 1.5400 | Val Acc: 27.85%\n",
      "Epoch 26/200 | Train Loss: 1.4998 | Train Acc: 31.21% | Val Loss: 1.5274 | Val Acc: 32.89%\n",
      "Epoch 27/200 | Train Loss: 1.4991 | Train Acc: 29.53% | Val Loss: 1.5368 | Val Acc: 27.85%\n",
      "Epoch 28/200 | Train Loss: 1.5028 | Train Acc: 32.21% | Val Loss: 1.5573 | Val Acc: 25.84%\n",
      "Epoch 29/200 | Train Loss: 1.4946 | Train Acc: 31.46% | Val Loss: 1.5406 | Val Acc: 31.88%\n",
      "Epoch 30/200 | Train Loss: 1.4988 | Train Acc: 30.87% | Val Loss: 1.5445 | Val Acc: 29.19%\n",
      "Epoch 31/200 | Train Loss: 1.4926 | Train Acc: 30.70% | Val Loss: 1.5399 | Val Acc: 26.85%\n",
      "Epoch 32/200 | Train Loss: 1.4910 | Train Acc: 29.11% | Val Loss: 1.5239 | Val Acc: 32.21%\n",
      "Epoch 33/200 | Train Loss: 1.4928 | Train Acc: 33.05% | Val Loss: 1.5218 | Val Acc: 32.55%\n",
      "Epoch 34/200 | Train Loss: 1.4842 | Train Acc: 31.54% | Val Loss: 1.5320 | Val Acc: 29.53%\n",
      "Epoch 35/200 | Train Loss: 1.4960 | Train Acc: 32.30% | Val Loss: 1.5198 | Val Acc: 26.17%\n",
      "Epoch 36/200 | Train Loss: 1.4807 | Train Acc: 32.97% | Val Loss: 1.5024 | Val Acc: 30.54%\n",
      "Epoch 37/200 | Train Loss: 1.4640 | Train Acc: 34.31% | Val Loss: 1.4873 | Val Acc: 35.57%\n",
      "Epoch 38/200 | Train Loss: 1.4637 | Train Acc: 33.89% | Val Loss: 1.4770 | Val Acc: 35.91%\n",
      "Epoch 39/200 | Train Loss: 1.4499 | Train Acc: 34.31% | Val Loss: 1.4963 | Val Acc: 37.58%\n",
      "Epoch 40/200 | Train Loss: 1.4545 | Train Acc: 34.90% | Val Loss: 1.4762 | Val Acc: 37.58%\n",
      "Epoch 41/200 | Train Loss: 1.4280 | Train Acc: 35.99% | Val Loss: 1.4815 | Val Acc: 40.60%\n",
      "Epoch 42/200 | Train Loss: 1.4274 | Train Acc: 35.57% | Val Loss: 1.4186 | Val Acc: 39.26%\n",
      "Epoch 43/200 | Train Loss: 1.4201 | Train Acc: 37.16% | Val Loss: 1.4244 | Val Acc: 40.94%\n",
      "Epoch 44/200 | Train Loss: 1.3978 | Train Acc: 39.43% | Val Loss: 1.4305 | Val Acc: 39.26%\n",
      "Epoch 45/200 | Train Loss: 1.3684 | Train Acc: 43.29% | Val Loss: 1.3882 | Val Acc: 41.28%\n",
      "Epoch 46/200 | Train Loss: 1.3770 | Train Acc: 41.28% | Val Loss: 1.3665 | Val Acc: 42.28%\n",
      "Epoch 47/200 | Train Loss: 1.3432 | Train Acc: 43.79% | Val Loss: 1.3783 | Val Acc: 43.96%\n",
      "Epoch 48/200 | Train Loss: 1.3398 | Train Acc: 43.62% | Val Loss: 1.3625 | Val Acc: 39.26%\n",
      "Epoch 49/200 | Train Loss: 1.3359 | Train Acc: 44.88% | Val Loss: 1.3282 | Val Acc: 40.60%\n",
      "Epoch 50/200 | Train Loss: 1.3269 | Train Acc: 43.96% | Val Loss: 1.4110 | Val Acc: 37.25%\n",
      "Epoch 51/200 | Train Loss: 1.3367 | Train Acc: 43.29% | Val Loss: 1.3810 | Val Acc: 42.28%\n",
      "Epoch 52/200 | Train Loss: 1.3100 | Train Acc: 46.31% | Val Loss: 1.3042 | Val Acc: 43.62%\n",
      "Epoch 53/200 | Train Loss: 1.3290 | Train Acc: 42.03% | Val Loss: 1.3357 | Val Acc: 42.28%\n",
      "Epoch 54/200 | Train Loss: 1.3369 | Train Acc: 45.22% | Val Loss: 1.3285 | Val Acc: 40.27%\n",
      "Epoch 55/200 | Train Loss: 1.3084 | Train Acc: 45.47% | Val Loss: 1.2990 | Val Acc: 41.61%\n",
      "Epoch 56/200 | Train Loss: 1.3171 | Train Acc: 44.04% | Val Loss: 1.3126 | Val Acc: 41.28%\n",
      "Epoch 57/200 | Train Loss: 1.2997 | Train Acc: 44.80% | Val Loss: 1.3198 | Val Acc: 42.28%\n",
      "Epoch 58/200 | Train Loss: 1.2987 | Train Acc: 45.64% | Val Loss: 1.2992 | Val Acc: 45.30%\n",
      "Epoch 59/200 | Train Loss: 1.3000 | Train Acc: 44.71% | Val Loss: 1.4677 | Val Acc: 35.57%\n",
      "Epoch 60/200 | Train Loss: 1.3236 | Train Acc: 43.46% | Val Loss: 1.3732 | Val Acc: 40.27%\n",
      "Epoch 61/200 | Train Loss: 1.2868 | Train Acc: 44.63% | Val Loss: 1.3696 | Val Acc: 40.94%\n",
      "Epoch 62/200 | Train Loss: 1.2754 | Train Acc: 46.39% | Val Loss: 1.3130 | Val Acc: 45.30%\n",
      "Epoch 63/200 | Train Loss: 1.2733 | Train Acc: 45.64% | Val Loss: 1.3130 | Val Acc: 42.95%\n",
      "Epoch 64/200 | Train Loss: 1.2694 | Train Acc: 46.22% | Val Loss: 1.2735 | Val Acc: 46.64%\n",
      "Epoch 65/200 | Train Loss: 1.2132 | Train Acc: 49.83% | Val Loss: 1.2609 | Val Acc: 44.30%\n",
      "Epoch 66/200 | Train Loss: 1.2837 | Train Acc: 46.81% | Val Loss: 1.4055 | Val Acc: 38.93%\n",
      "Epoch 67/200 | Train Loss: 1.2786 | Train Acc: 45.30% | Val Loss: 1.3171 | Val Acc: 46.31%\n",
      "Epoch 68/200 | Train Loss: 1.2442 | Train Acc: 48.49% | Val Loss: 1.2542 | Val Acc: 50.00%\n",
      "Epoch 69/200 | Train Loss: 1.2652 | Train Acc: 48.74% | Val Loss: 1.2734 | Val Acc: 46.31%\n",
      "Epoch 70/200 | Train Loss: 1.2832 | Train Acc: 44.71% | Val Loss: 1.2637 | Val Acc: 47.65%\n",
      "Epoch 71/200 | Train Loss: 1.1910 | Train Acc: 52.01% | Val Loss: 1.3348 | Val Acc: 45.30%\n",
      "Epoch 72/200 | Train Loss: 1.1930 | Train Acc: 52.18% | Val Loss: 1.2213 | Val Acc: 50.00%\n",
      "Epoch 73/200 | Train Loss: 1.1716 | Train Acc: 50.67% | Val Loss: 1.2063 | Val Acc: 50.34%\n",
      "Epoch 74/200 | Train Loss: 1.1536 | Train Acc: 52.85% | Val Loss: 1.2521 | Val Acc: 46.64%\n",
      "Epoch 75/200 | Train Loss: 1.2009 | Train Acc: 49.08% | Val Loss: 1.2627 | Val Acc: 49.66%\n",
      "Epoch 76/200 | Train Loss: 1.1354 | Train Acc: 54.36% | Val Loss: 1.2766 | Val Acc: 47.99%\n",
      "Epoch 77/200 | Train Loss: 1.1377 | Train Acc: 53.44% | Val Loss: 1.2379 | Val Acc: 49.33%\n",
      "Epoch 78/200 | Train Loss: 1.1562 | Train Acc: 53.10% | Val Loss: 1.1804 | Val Acc: 50.00%\n",
      "Epoch 79/200 | Train Loss: 1.1668 | Train Acc: 52.77% | Val Loss: 1.3966 | Val Acc: 42.62%\n",
      "Epoch 80/200 | Train Loss: 1.1824 | Train Acc: 51.85% | Val Loss: 1.2416 | Val Acc: 48.99%\n",
      "Epoch 81/200 | Train Loss: 1.2079 | Train Acc: 49.24% | Val Loss: 1.3261 | Val Acc: 42.62%\n",
      "Epoch 82/200 | Train Loss: 1.1493 | Train Acc: 52.60% | Val Loss: 1.2204 | Val Acc: 52.35%\n",
      "Epoch 83/200 | Train Loss: 1.1247 | Train Acc: 52.68% | Val Loss: 1.2651 | Val Acc: 47.99%\n",
      "Epoch 84/200 | Train Loss: 1.1390 | Train Acc: 52.18% | Val Loss: 1.1748 | Val Acc: 53.02%\n",
      "Epoch 85/200 | Train Loss: 1.2552 | Train Acc: 48.66% | Val Loss: 1.2588 | Val Acc: 46.98%\n",
      "Epoch 86/200 | Train Loss: 1.2716 | Train Acc: 48.15% | Val Loss: 1.1895 | Val Acc: 52.01%\n",
      "Epoch 87/200 | Train Loss: 1.1596 | Train Acc: 52.77% | Val Loss: 1.2267 | Val Acc: 49.66%\n",
      "Epoch 88/200 | Train Loss: 1.1496 | Train Acc: 54.78% | Val Loss: 1.2252 | Val Acc: 48.32%\n",
      "Epoch 89/200 | Train Loss: 1.1405 | Train Acc: 55.70% | Val Loss: 1.1821 | Val Acc: 48.99%\n",
      "Epoch 90/200 | Train Loss: 1.1827 | Train Acc: 52.10% | Val Loss: 1.3249 | Val Acc: 44.63%\n",
      "Epoch 91/200 | Train Loss: 1.1251 | Train Acc: 54.45% | Val Loss: 1.2807 | Val Acc: 47.32%\n",
      "Epoch 92/200 | Train Loss: 1.2061 | Train Acc: 50.50% | Val Loss: 1.1680 | Val Acc: 53.69%\n",
      "Epoch 93/200 | Train Loss: 1.1756 | Train Acc: 53.44% | Val Loss: 1.3327 | Val Acc: 45.30%\n",
      "Epoch 94/200 | Train Loss: 1.2396 | Train Acc: 47.73% | Val Loss: 1.3290 | Val Acc: 47.32%\n",
      "Epoch 95/200 | Train Loss: 1.1360 | Train Acc: 55.45% | Val Loss: 1.1987 | Val Acc: 53.69%\n",
      "Epoch 96/200 | Train Loss: 1.0700 | Train Acc: 57.72% | Val Loss: 1.1610 | Val Acc: 52.68%\n",
      "Epoch 97/200 | Train Loss: 1.0886 | Train Acc: 56.21% | Val Loss: 1.1546 | Val Acc: 54.03%\n",
      "Epoch 98/200 | Train Loss: 1.0704 | Train Acc: 57.38% | Val Loss: 1.1209 | Val Acc: 54.70%\n",
      "Epoch 99/200 | Train Loss: 1.0857 | Train Acc: 57.55% | Val Loss: 1.1672 | Val Acc: 51.34%\n",
      "Epoch 100/200 | Train Loss: 1.0647 | Train Acc: 57.89% | Val Loss: 1.2366 | Val Acc: 48.66%\n",
      "Epoch 101/200 | Train Loss: 1.1130 | Train Acc: 54.61% | Val Loss: 1.2211 | Val Acc: 51.68%\n",
      "Epoch 102/200 | Train Loss: 1.1047 | Train Acc: 54.36% | Val Loss: 1.1199 | Val Acc: 53.36%\n",
      "Epoch 103/200 | Train Loss: 1.0987 | Train Acc: 56.38% | Val Loss: 1.1492 | Val Acc: 53.36%\n",
      "Epoch 104/200 | Train Loss: 1.0332 | Train Acc: 58.05% | Val Loss: 1.1991 | Val Acc: 51.68%\n",
      "Epoch 105/200 | Train Loss: 1.0725 | Train Acc: 57.80% | Val Loss: 1.1626 | Val Acc: 54.36%\n",
      "Epoch 106/200 | Train Loss: 1.1279 | Train Acc: 56.80% | Val Loss: 1.2358 | Val Acc: 50.34%\n",
      "Epoch 107/200 | Train Loss: 1.1885 | Train Acc: 50.17% | Val Loss: 1.1912 | Val Acc: 52.01%\n",
      "Epoch 108/200 | Train Loss: 1.0823 | Train Acc: 58.22% | Val Loss: 1.1339 | Val Acc: 52.35%\n",
      "Epoch 109/200 | Train Loss: 1.0508 | Train Acc: 57.97% | Val Loss: 1.1667 | Val Acc: 53.36%\n",
      "Epoch 110/200 | Train Loss: 1.0276 | Train Acc: 60.32% | Val Loss: 1.4752 | Val Acc: 46.31%\n",
      "Epoch 111/200 | Train Loss: 1.2025 | Train Acc: 51.34% | Val Loss: 1.2325 | Val Acc: 51.01%\n",
      "Epoch 112/200 | Train Loss: 1.1254 | Train Acc: 53.52% | Val Loss: 1.2005 | Val Acc: 52.68%\n",
      "Epoch 113/200 | Train Loss: 1.0644 | Train Acc: 56.63% | Val Loss: 1.1797 | Val Acc: 54.03%\n",
      "Epoch 114/200 | Train Loss: 1.0511 | Train Acc: 59.98% | Val Loss: 1.2232 | Val Acc: 52.68%\n",
      "Epoch 115/200 | Train Loss: 1.1397 | Train Acc: 53.78% | Val Loss: 1.2056 | Val Acc: 53.36%\n",
      "Epoch 116/200 | Train Loss: 1.2031 | Train Acc: 51.93% | Val Loss: 1.2716 | Val Acc: 48.99%\n",
      "Epoch 117/200 | Train Loss: 1.1837 | Train Acc: 51.01% | Val Loss: 1.2417 | Val Acc: 48.99%\n",
      "Epoch 118/200 | Train Loss: 1.1341 | Train Acc: 54.19% | Val Loss: 1.1797 | Val Acc: 53.36%\n",
      "Epoch 119/200 | Train Loss: 1.1592 | Train Acc: 53.69% | Val Loss: 1.2017 | Val Acc: 50.34%\n",
      "Epoch 120/200 | Train Loss: 1.1262 | Train Acc: 55.20% | Val Loss: 1.4124 | Val Acc: 43.96%\n",
      "Epoch 121/200 | Train Loss: 1.2535 | Train Acc: 48.24% | Val Loss: 1.3239 | Val Acc: 44.97%\n",
      "Epoch 122/200 | Train Loss: 1.2162 | Train Acc: 51.34% | Val Loss: 1.2672 | Val Acc: 46.98%\n",
      "Early stopping at epoch 121. Best val loss: 1.1199\n",
      "Epoch 123/200 | Train Loss: 1.1786 | Train Acc: 51.68% | Val Loss: 1.1941 | Val Acc: 53.02%\n",
      "Early stopping at epoch 122. Best val loss: 1.1199\n",
      "Epoch 124/200 | Train Loss: 1.1906 | Train Acc: 51.85% | Val Loss: 1.2303 | Val Acc: 48.66%\n",
      "Early stopping at epoch 123. Best val loss: 1.1199\n",
      "Epoch 125/200 | Train Loss: 1.1562 | Train Acc: 54.53% | Val Loss: 1.3970 | Val Acc: 45.30%\n",
      "Early stopping at epoch 124. Best val loss: 1.1199\n",
      "Epoch 126/200 | Train Loss: 1.1901 | Train Acc: 49.50% | Val Loss: 1.1991 | Val Acc: 48.99%\n",
      "Early stopping at epoch 125. Best val loss: 1.1199\n",
      "Epoch 127/200 | Train Loss: 1.1099 | Train Acc: 55.03% | Val Loss: 1.1847 | Val Acc: 50.00%\n",
      "Early stopping at epoch 126. Best val loss: 1.1199\n",
      "Epoch 128/200 | Train Loss: 1.1052 | Train Acc: 57.38% | Val Loss: 1.1612 | Val Acc: 53.69%\n",
      "Early stopping at epoch 127. Best val loss: 1.1199\n",
      "Epoch 129/200 | Train Loss: 1.0816 | Train Acc: 57.30% | Val Loss: 1.1691 | Val Acc: 51.34%\n",
      "Early stopping at epoch 128. Best val loss: 1.1199\n",
      "Epoch 130/200 | Train Loss: 1.1220 | Train Acc: 55.03% | Val Loss: 1.3016 | Val Acc: 45.97%\n",
      "Early stopping at epoch 129. Best val loss: 1.1199\n",
      "Epoch 131/200 | Train Loss: 1.1118 | Train Acc: 55.54% | Val Loss: 1.1877 | Val Acc: 52.35%\n",
      "Early stopping at epoch 130. Best val loss: 1.1199\n",
      "Epoch 132/200 | Train Loss: 1.1305 | Train Acc: 54.53% | Val Loss: 1.2305 | Val Acc: 51.34%\n",
      "Early stopping at epoch 131. Best val loss: 1.1199\n",
      "Epoch 133/200 | Train Loss: 1.1099 | Train Acc: 55.03% | Val Loss: 1.2227 | Val Acc: 48.99%\n",
      "Early stopping at epoch 132. Best val loss: 1.1199\n",
      "Epoch 134/200 | Train Loss: 1.0793 | Train Acc: 58.72% | Val Loss: 1.1613 | Val Acc: 50.67%\n",
      "Early stopping at epoch 133. Best val loss: 1.1199\n",
      "Epoch 135/200 | Train Loss: 1.0683 | Train Acc: 58.64% | Val Loss: 1.1846 | Val Acc: 51.01%\n",
      "Early stopping at epoch 134. Best val loss: 1.1199\n",
      "Epoch 136/200 | Train Loss: 1.0811 | Train Acc: 57.97% | Val Loss: 1.1459 | Val Acc: 55.37%\n",
      "Early stopping at epoch 135. Best val loss: 1.1199\n",
      "Epoch 137/200 | Train Loss: 1.0673 | Train Acc: 58.47% | Val Loss: 1.1509 | Val Acc: 51.34%\n",
      "Early stopping at epoch 136. Best val loss: 1.1199\n",
      "Epoch 138/200 | Train Loss: 1.0559 | Train Acc: 60.07% | Val Loss: 1.1697 | Val Acc: 51.68%\n",
      "Early stopping at epoch 137. Best val loss: 1.1199\n",
      "Epoch 139/200 | Train Loss: 1.0728 | Train Acc: 56.71% | Val Loss: 1.1331 | Val Acc: 54.36%\n",
      "Early stopping at epoch 138. Best val loss: 1.1199\n",
      "Epoch 140/200 | Train Loss: 1.1052 | Train Acc: 55.96% | Val Loss: 1.1740 | Val Acc: 52.68%\n",
      "Early stopping at epoch 139. Best val loss: 1.1199\n",
      "Epoch 141/200 | Train Loss: 1.0811 | Train Acc: 56.88% | Val Loss: 1.1427 | Val Acc: 52.68%\n",
      "Early stopping at epoch 140. Best val loss: 1.1199\n",
      "Epoch 142/200 | Train Loss: 1.0568 | Train Acc: 58.39% | Val Loss: 1.1597 | Val Acc: 54.03%\n",
      "Early stopping at epoch 141. Best val loss: 1.1199\n",
      "Epoch 143/200 | Train Loss: 1.0589 | Train Acc: 59.23% | Val Loss: 1.1702 | Val Acc: 54.03%\n",
      "Early stopping at epoch 142. Best val loss: 1.1199\n",
      "Epoch 144/200 | Train Loss: 1.0676 | Train Acc: 56.88% | Val Loss: 1.1391 | Val Acc: 53.36%\n",
      "Early stopping at epoch 143. Best val loss: 1.1199\n",
      "Epoch 145/200 | Train Loss: 1.0573 | Train Acc: 58.22% | Val Loss: 1.1271 | Val Acc: 53.69%\n",
      "Early stopping at epoch 144. Best val loss: 1.1199\n",
      "Epoch 146/200 | Train Loss: 1.0495 | Train Acc: 58.72% | Val Loss: 1.1402 | Val Acc: 52.68%\n",
      "Early stopping at epoch 145. Best val loss: 1.1199\n",
      "Epoch 147/200 | Train Loss: 1.0592 | Train Acc: 58.05% | Val Loss: 1.1478 | Val Acc: 55.37%\n",
      "Early stopping at epoch 146. Best val loss: 1.1199\n",
      "Epoch 148/200 | Train Loss: 1.0361 | Train Acc: 58.56% | Val Loss: 1.1181 | Val Acc: 55.37%\n",
      "Epoch 149/200 | Train Loss: 1.0411 | Train Acc: 58.98% | Val Loss: 1.1910 | Val Acc: 51.34%\n",
      "Epoch 150/200 | Train Loss: 1.0493 | Train Acc: 58.56% | Val Loss: 1.1724 | Val Acc: 53.69%\n",
      "Epoch 151/200 | Train Loss: 1.0488 | Train Acc: 58.64% | Val Loss: 1.1572 | Val Acc: 53.69%\n",
      "Epoch 152/200 | Train Loss: 1.0310 | Train Acc: 59.98% | Val Loss: 1.1145 | Val Acc: 56.38%\n",
      "Epoch 153/200 | Train Loss: 1.0254 | Train Acc: 60.23% | Val Loss: 1.1244 | Val Acc: 54.70%\n",
      "Epoch 154/200 | Train Loss: 1.0239 | Train Acc: 61.49% | Val Loss: 1.1686 | Val Acc: 53.69%\n",
      "Epoch 155/200 | Train Loss: 1.0421 | Train Acc: 59.14% | Val Loss: 1.1104 | Val Acc: 56.71%\n",
      "Epoch 156/200 | Train Loss: 1.0161 | Train Acc: 61.58% | Val Loss: 1.1178 | Val Acc: 54.70%\n",
      "Epoch 157/200 | Train Loss: 1.0231 | Train Acc: 60.07% | Val Loss: 1.1840 | Val Acc: 52.35%\n",
      "Epoch 158/200 | Train Loss: 1.0201 | Train Acc: 60.40% | Val Loss: 1.1041 | Val Acc: 55.03%\n",
      "Epoch 159/200 | Train Loss: 1.0096 | Train Acc: 62.50% | Val Loss: 1.1038 | Val Acc: 56.04%\n",
      "Epoch 160/200 | Train Loss: 1.0167 | Train Acc: 59.73% | Val Loss: 1.1229 | Val Acc: 54.70%\n",
      "Epoch 161/200 | Train Loss: 1.0150 | Train Acc: 60.74% | Val Loss: 1.1123 | Val Acc: 57.05%\n",
      "Epoch 162/200 | Train Loss: 1.0261 | Train Acc: 60.49% | Val Loss: 1.1689 | Val Acc: 52.35%\n",
      "Epoch 163/200 | Train Loss: 1.0286 | Train Acc: 59.14% | Val Loss: 1.0917 | Val Acc: 55.03%\n",
      "Epoch 164/200 | Train Loss: 0.9990 | Train Acc: 60.74% | Val Loss: 1.0780 | Val Acc: 56.71%\n",
      "Epoch 165/200 | Train Loss: 0.9900 | Train Acc: 60.82% | Val Loss: 1.1313 | Val Acc: 55.03%\n",
      "Epoch 166/200 | Train Loss: 0.9909 | Train Acc: 60.40% | Val Loss: 1.0801 | Val Acc: 56.38%\n",
      "Epoch 167/200 | Train Loss: 0.9771 | Train Acc: 61.83% | Val Loss: 1.1111 | Val Acc: 56.71%\n",
      "Epoch 168/200 | Train Loss: 0.9816 | Train Acc: 60.49% | Val Loss: 1.0834 | Val Acc: 56.04%\n",
      "Epoch 169/200 | Train Loss: 0.9747 | Train Acc: 61.91% | Val Loss: 1.0784 | Val Acc: 57.05%\n",
      "Epoch 170/200 | Train Loss: 0.9765 | Train Acc: 61.58% | Val Loss: 1.0742 | Val Acc: 57.38%\n",
      "Epoch 171/200 | Train Loss: 0.9790 | Train Acc: 61.74% | Val Loss: 1.0710 | Val Acc: 58.05%\n",
      "Epoch 172/200 | Train Loss: 0.9681 | Train Acc: 61.58% | Val Loss: 1.0781 | Val Acc: 57.38%\n",
      "Epoch 173/200 | Train Loss: 0.9679 | Train Acc: 61.91% | Val Loss: 1.0793 | Val Acc: 56.71%\n",
      "Epoch 174/200 | Train Loss: 0.9646 | Train Acc: 61.91% | Val Loss: 1.0753 | Val Acc: 57.38%\n",
      "Epoch 175/200 | Train Loss: 0.9689 | Train Acc: 60.99% | Val Loss: 1.1287 | Val Acc: 55.70%\n",
      "Epoch 176/200 | Train Loss: 0.9776 | Train Acc: 61.83% | Val Loss: 1.0803 | Val Acc: 57.72%\n",
      "Epoch 177/200 | Train Loss: 0.9611 | Train Acc: 61.41% | Val Loss: 1.0839 | Val Acc: 56.71%\n",
      "Epoch 178/200 | Train Loss: 0.9628 | Train Acc: 61.91% | Val Loss: 1.0682 | Val Acc: 58.39%\n",
      "Epoch 179/200 | Train Loss: 0.9655 | Train Acc: 61.33% | Val Loss: 1.0677 | Val Acc: 58.39%\n",
      "Epoch 180/200 | Train Loss: 0.9624 | Train Acc: 61.33% | Val Loss: 1.0680 | Val Acc: 57.72%\n",
      "Epoch 181/200 | Train Loss: 0.9545 | Train Acc: 61.83% | Val Loss: 1.0705 | Val Acc: 57.72%\n",
      "Epoch 182/200 | Train Loss: 0.9542 | Train Acc: 61.74% | Val Loss: 1.0589 | Val Acc: 58.72%\n",
      "Epoch 183/200 | Train Loss: 0.9565 | Train Acc: 62.42% | Val Loss: 1.0713 | Val Acc: 58.39%\n",
      "Epoch 184/200 | Train Loss: 0.9554 | Train Acc: 61.58% | Val Loss: 1.0618 | Val Acc: 58.39%\n",
      "Epoch 185/200 | Train Loss: 0.9489 | Train Acc: 62.00% | Val Loss: 1.0657 | Val Acc: 58.05%\n",
      "Epoch 186/200 | Train Loss: 0.9569 | Train Acc: 61.49% | Val Loss: 1.0603 | Val Acc: 58.39%\n",
      "Epoch 187/200 | Train Loss: 0.9508 | Train Acc: 62.16% | Val Loss: 1.0654 | Val Acc: 58.05%\n",
      "Epoch 188/200 | Train Loss: 0.9513 | Train Acc: 61.91% | Val Loss: 1.0638 | Val Acc: 58.72%\n",
      "Epoch 189/200 | Train Loss: 0.9479 | Train Acc: 62.00% | Val Loss: 1.0641 | Val Acc: 58.39%\n",
      "Epoch 190/200 | Train Loss: 0.9470 | Train Acc: 62.00% | Val Loss: 1.0599 | Val Acc: 58.39%\n",
      "Epoch 191/200 | Train Loss: 0.9531 | Train Acc: 62.00% | Val Loss: 1.0610 | Val Acc: 59.06%\n",
      "Epoch 192/200 | Train Loss: 0.9495 | Train Acc: 62.00% | Val Loss: 1.0628 | Val Acc: 58.39%\n",
      "Epoch 193/200 | Train Loss: 0.9489 | Train Acc: 62.16% | Val Loss: 1.0613 | Val Acc: 58.39%\n",
      "Epoch 194/200 | Train Loss: 0.9469 | Train Acc: 62.16% | Val Loss: 1.0612 | Val Acc: 58.05%\n",
      "Epoch 195/200 | Train Loss: 0.9485 | Train Acc: 62.08% | Val Loss: 1.0618 | Val Acc: 58.72%\n",
      "Epoch 196/200 | Train Loss: 0.9442 | Train Acc: 62.16% | Val Loss: 1.0630 | Val Acc: 59.06%\n",
      "Epoch 197/200 | Train Loss: 0.9483 | Train Acc: 62.08% | Val Loss: 1.0616 | Val Acc: 59.06%\n",
      "Epoch 198/200 | Train Loss: 0.9478 | Train Acc: 62.08% | Val Loss: 1.0613 | Val Acc: 58.39%\n",
      "Epoch 199/200 | Train Loss: 0.9464 | Train Acc: 62.16% | Val Loss: 1.0612 | Val Acc: 58.39%\n",
      "Epoch 200/200 | Train Loss: 0.9505 | Train Acc: 62.16% | Val Loss: 1.0612 | Val Acc: 58.39%\n"
     ]
    }
   ],
   "source": [
    "best_model = train(model, train_loader,val_loader ,criterion, optimizer,scheduler, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3650 | Test Accuracy: 88.44%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dla2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
