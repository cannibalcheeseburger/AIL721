{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/kash/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "EPOCHS=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path =\"Datasets/TrainData.csv\"\n",
    "test_path = \"Datasets/TestLabels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_df = pd.read_csv(train_path)  # Change to actual file path\n",
    "train_texts = train_df[\"Text\"].astype(str).tolist()\n",
    "train_labels = train_df[\"Category\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_path)  # Change to actual file path\n",
    "test_texts = test_df[\"Text\"].astype(str).tolist()\n",
    "test_labels = test_df[\"Label - (business, tech, politics, sport, entertainment)\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokenized = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    #filtered = [word for word in tokenized]\n",
    "    filtered = [word for word in tokenized if word not in punctuation]\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [preprocess_text(text) for text in train_texts]\n",
    "test_tokenized = [preprocess_text(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word: idx + 1 for idx, word in enumerate(set(word for text in tokenized_texts for word in text))}\n",
    "vocab_size = len(vocab) + 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[\"<OOV>\"] = len(vocab) + 1\n",
    "\n",
    "def text_to_indices(text, vocab, max_len):\n",
    "    indices = [vocab.get(word, vocab[\"<OOV>\"]) for word in text[:max_len]]\n",
    "    if len(indices) < max_len:\n",
    "        indices += [0] * (max_len - len(indices)) # Padding with 0\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "test_labels = label_encoder.transform(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(inner_array) for inner_array in tokenized_texts]\n",
    "max_len = int(np.percentile(lengths, 97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([text_to_indices(text, vocab, max_len) for text in tokenized_texts])\n",
    "y_train = np.array(train_labels)\n",
    "X_test = np.array([text_to_indices(text, vocab, max_len) for text in test_tokenized])\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)  # Use long type for embedding lookup\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(X_train, y_train)\n",
    "val_dataset = TextDataset(X_val, y_val)\n",
    "test_dataset = TextDataset(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes, out_size=64, hidden_layer=64):\n",
    "        super(CLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # Embedding layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=out_size, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_size)  # Batch normalization for convolution output\n",
    "        self.lstm = nn.LSTM(input_size=out_size, hidden_size=out_size,\n",
    "                            batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        \n",
    "        # Add Layer Normalization before Attention\n",
    "        self.layer_norm = nn.LayerNorm(out_size * 2)  # Normalize over features (bidirectional LSTM output)\n",
    "\n",
    "        self.attention = nn.Linear(out_size * 2, 1)  \n",
    "        self.fc1 = nn.Linear(out_size * 2, hidden_layer)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.fc2 = nn.Linear(hidden_layer, num_classes)\n",
    "        self.fc3 = nn.Linear(hidden_layer//4, num_classes)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) \n",
    "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, embedding_dim, seq_len)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Apply Batch Normalization\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 2, 1)  # Shape: (batch_size, seq_len, out_channels)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x)  \n",
    "        \n",
    "        # Apply Layer Normalization before Attention\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "\n",
    "        attention_scores = self.attention(torch.tanh(lstm_out))  # Optional nonlinearity\n",
    "        attn_weights = torch.softmax(attention_scores.squeeze(-1), dim=1)  # Shape: (batch_size, seq_len)\n",
    "        \n",
    "        context_vector = torch.sum(attn_weights.unsqueeze(-1) * lstm_out, dim=1)  # Shape: (batch_size, out_size * 2)\n",
    "        \n",
    "        x = torch.relu(self.fc1(context_vector))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)  \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10):\n",
    "    model.train()\n",
    "    patience = 20  \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0  \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += y_batch.size(0)\n",
    "            correct_train += (predicted == y_batch).sum().item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        avg_val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(avg_val_loss)  # Adjust learning rate based on validation loss\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}% | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0  # Reset patience counter\n",
    "            best_model = model.state_dict()  # Save best model\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best val loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "    return best_model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kash/miniconda/envs/dla2/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 32\n",
    "num_classes = len(set(y_train))\n",
    "model = CLSTM(vocab_size=vocab_size,\n",
    "              embedding_dim=embedding_dim,\n",
    "              num_classes=num_classes,\n",
    "              out_size=128,\n",
    "              hidden_layer=64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kash/miniconda/envs/dla2/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.002, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=10, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 1.6137 | Train Acc: 21.56% | Val Loss: 1.5236 | Val Acc: 30.20%\n",
      "Epoch 2/200 | Train Loss: 1.5592 | Train Acc: 27.60% | Val Loss: 1.4879 | Val Acc: 29.53%\n",
      "Epoch 3/200 | Train Loss: 1.5346 | Train Acc: 29.28% | Val Loss: 1.4758 | Val Acc: 27.85%\n",
      "Epoch 4/200 | Train Loss: 1.5468 | Train Acc: 27.77% | Val Loss: 1.4985 | Val Acc: 29.53%\n",
      "Epoch 5/200 | Train Loss: 1.5294 | Train Acc: 29.61% | Val Loss: 1.4437 | Val Acc: 29.87%\n",
      "Epoch 6/200 | Train Loss: 1.5126 | Train Acc: 30.79% | Val Loss: 1.4605 | Val Acc: 33.22%\n",
      "Epoch 7/200 | Train Loss: 1.5233 | Train Acc: 29.87% | Val Loss: 1.4447 | Val Acc: 30.20%\n",
      "Epoch 8/200 | Train Loss: 1.5054 | Train Acc: 31.71% | Val Loss: 1.4312 | Val Acc: 35.23%\n",
      "Epoch 9/200 | Train Loss: 1.5012 | Train Acc: 33.05% | Val Loss: 1.4566 | Val Acc: 36.58%\n",
      "Epoch 10/200 | Train Loss: 1.4971 | Train Acc: 34.40% | Val Loss: 1.4404 | Val Acc: 37.58%\n",
      "Epoch 11/200 | Train Loss: 1.5007 | Train Acc: 32.55% | Val Loss: 1.4226 | Val Acc: 33.22%\n",
      "Epoch 12/200 | Train Loss: 1.4813 | Train Acc: 33.81% | Val Loss: 1.4372 | Val Acc: 37.25%\n",
      "Epoch 13/200 | Train Loss: 1.4795 | Train Acc: 33.81% | Val Loss: 1.4952 | Val Acc: 33.89%\n",
      "Epoch 14/200 | Train Loss: 1.4853 | Train Acc: 33.31% | Val Loss: 1.4157 | Val Acc: 40.27%\n",
      "Epoch 15/200 | Train Loss: 1.4672 | Train Acc: 36.33% | Val Loss: 1.5074 | Val Acc: 32.55%\n",
      "Epoch 16/200 | Train Loss: 1.4585 | Train Acc: 37.33% | Val Loss: 1.4159 | Val Acc: 44.97%\n",
      "Epoch 17/200 | Train Loss: 1.4181 | Train Acc: 40.18% | Val Loss: 1.3974 | Val Acc: 40.94%\n",
      "Epoch 18/200 | Train Loss: 1.4185 | Train Acc: 38.59% | Val Loss: 1.5369 | Val Acc: 30.20%\n",
      "Epoch 19/200 | Train Loss: 1.4625 | Train Acc: 35.15% | Val Loss: 1.2986 | Val Acc: 46.98%\n",
      "Epoch 20/200 | Train Loss: 1.4616 | Train Acc: 36.83% | Val Loss: 1.3254 | Val Acc: 44.97%\n",
      "Epoch 21/200 | Train Loss: 1.4001 | Train Acc: 41.61% | Val Loss: 1.3124 | Val Acc: 55.37%\n",
      "Epoch 22/200 | Train Loss: 1.3456 | Train Acc: 47.65% | Val Loss: 1.2749 | Val Acc: 52.68%\n",
      "Epoch 23/200 | Train Loss: 1.3044 | Train Acc: 48.41% | Val Loss: 1.2250 | Val Acc: 53.02%\n",
      "Epoch 24/200 | Train Loss: 1.2793 | Train Acc: 47.99% | Val Loss: 1.2372 | Val Acc: 44.97%\n",
      "Epoch 25/200 | Train Loss: 1.2531 | Train Acc: 51.51% | Val Loss: 1.1310 | Val Acc: 48.66%\n",
      "Epoch 26/200 | Train Loss: 1.1684 | Train Acc: 54.36% | Val Loss: 1.1731 | Val Acc: 51.34%\n",
      "Epoch 27/200 | Train Loss: 1.1604 | Train Acc: 53.69% | Val Loss: 1.1709 | Val Acc: 47.32%\n",
      "Epoch 28/200 | Train Loss: 1.1061 | Train Acc: 56.38% | Val Loss: 1.2203 | Val Acc: 46.31%\n",
      "Epoch 29/200 | Train Loss: 1.0808 | Train Acc: 55.54% | Val Loss: 1.2469 | Val Acc: 47.99%\n",
      "Epoch 30/200 | Train Loss: 1.1806 | Train Acc: 53.10% | Val Loss: 1.2997 | Val Acc: 51.68%\n",
      "Epoch 31/200 | Train Loss: 1.0829 | Train Acc: 57.80% | Val Loss: 1.1101 | Val Acc: 57.05%\n",
      "Epoch 32/200 | Train Loss: 1.0655 | Train Acc: 59.14% | Val Loss: 1.0741 | Val Acc: 56.38%\n",
      "Epoch 33/200 | Train Loss: 1.0109 | Train Acc: 60.99% | Val Loss: 0.9178 | Val Acc: 61.41%\n",
      "Epoch 34/200 | Train Loss: 0.9667 | Train Acc: 63.09% | Val Loss: 1.0111 | Val Acc: 59.73%\n",
      "Epoch 35/200 | Train Loss: 0.9291 | Train Acc: 65.44% | Val Loss: 0.9937 | Val Acc: 60.74%\n",
      "Epoch 36/200 | Train Loss: 0.8598 | Train Acc: 67.87% | Val Loss: 0.9190 | Val Acc: 61.74%\n",
      "Epoch 37/200 | Train Loss: 0.8139 | Train Acc: 70.81% | Val Loss: 0.9279 | Val Acc: 64.77%\n",
      "Epoch 38/200 | Train Loss: 0.8564 | Train Acc: 66.95% | Val Loss: 1.1217 | Val Acc: 60.40%\n",
      "Epoch 39/200 | Train Loss: 0.8144 | Train Acc: 68.79% | Val Loss: 1.0454 | Val Acc: 55.37%\n",
      "Epoch 40/200 | Train Loss: 0.8626 | Train Acc: 66.53% | Val Loss: 0.9491 | Val Acc: 61.41%\n",
      "Epoch 41/200 | Train Loss: 0.7917 | Train Acc: 70.13% | Val Loss: 0.8440 | Val Acc: 66.11%\n",
      "Epoch 42/200 | Train Loss: 0.7660 | Train Acc: 72.57% | Val Loss: 0.8308 | Val Acc: 67.11%\n",
      "Epoch 43/200 | Train Loss: 0.6992 | Train Acc: 73.74% | Val Loss: 0.8991 | Val Acc: 64.09%\n",
      "Epoch 44/200 | Train Loss: 0.6759 | Train Acc: 73.66% | Val Loss: 0.8743 | Val Acc: 65.44%\n",
      "Epoch 45/200 | Train Loss: 0.6474 | Train Acc: 75.92% | Val Loss: 0.9658 | Val Acc: 65.77%\n",
      "Epoch 46/200 | Train Loss: 0.6040 | Train Acc: 77.35% | Val Loss: 0.8570 | Val Acc: 69.13%\n",
      "Epoch 47/200 | Train Loss: 0.5514 | Train Acc: 79.70% | Val Loss: 0.8177 | Val Acc: 68.79%\n",
      "Epoch 48/200 | Train Loss: 0.5630 | Train Acc: 79.70% | Val Loss: 0.9561 | Val Acc: 63.76%\n",
      "Epoch 49/200 | Train Loss: 0.4790 | Train Acc: 82.97% | Val Loss: 0.8580 | Val Acc: 66.78%\n",
      "Epoch 50/200 | Train Loss: 0.4568 | Train Acc: 84.65% | Val Loss: 0.8392 | Val Acc: 70.81%\n",
      "Epoch 51/200 | Train Loss: 0.4345 | Train Acc: 85.99% | Val Loss: 1.0224 | Val Acc: 68.46%\n",
      "Epoch 52/200 | Train Loss: 0.5644 | Train Acc: 78.44% | Val Loss: 0.9805 | Val Acc: 67.79%\n",
      "Epoch 53/200 | Train Loss: 0.5693 | Train Acc: 79.78% | Val Loss: 0.9038 | Val Acc: 70.81%\n",
      "Epoch 54/200 | Train Loss: 0.4390 | Train Acc: 84.90% | Val Loss: 0.8653 | Val Acc: 71.14%\n",
      "Epoch 55/200 | Train Loss: 0.3828 | Train Acc: 86.83% | Val Loss: 0.8673 | Val Acc: 72.15%\n",
      "Epoch 56/200 | Train Loss: 0.3296 | Train Acc: 90.02% | Val Loss: 0.8003 | Val Acc: 71.14%\n",
      "Epoch 57/200 | Train Loss: 0.2793 | Train Acc: 91.11% | Val Loss: 0.9136 | Val Acc: 71.14%\n",
      "Epoch 58/200 | Train Loss: 0.2546 | Train Acc: 92.95% | Val Loss: 0.8363 | Val Acc: 73.49%\n",
      "Epoch 59/200 | Train Loss: 0.2182 | Train Acc: 94.21% | Val Loss: 0.9571 | Val Acc: 71.81%\n",
      "Epoch 60/200 | Train Loss: 0.1899 | Train Acc: 94.30% | Val Loss: 0.9174 | Val Acc: 72.15%\n",
      "Epoch 61/200 | Train Loss: 0.1482 | Train Acc: 96.39% | Val Loss: 0.9648 | Val Acc: 72.15%\n",
      "Epoch 62/200 | Train Loss: 0.1199 | Train Acc: 97.73% | Val Loss: 1.0139 | Val Acc: 71.48%\n",
      "Epoch 63/200 | Train Loss: 0.0933 | Train Acc: 98.83% | Val Loss: 1.0066 | Val Acc: 71.14%\n",
      "Epoch 64/200 | Train Loss: 0.0752 | Train Acc: 99.08% | Val Loss: 0.9946 | Val Acc: 72.15%\n",
      "Epoch 65/200 | Train Loss: 0.0612 | Train Acc: 99.41% | Val Loss: 1.0398 | Val Acc: 71.14%\n",
      "Epoch 66/200 | Train Loss: 0.0538 | Train Acc: 99.16% | Val Loss: 1.0723 | Val Acc: 72.15%\n",
      "Epoch 67/200 | Train Loss: 0.0387 | Train Acc: 99.66% | Val Loss: 1.1475 | Val Acc: 73.15%\n",
      "Epoch 68/200 | Train Loss: 0.0330 | Train Acc: 99.75% | Val Loss: 1.1525 | Val Acc: 72.15%\n",
      "Epoch 69/200 | Train Loss: 0.0251 | Train Acc: 99.75% | Val Loss: 1.1773 | Val Acc: 72.15%\n",
      "Epoch 70/200 | Train Loss: 0.0208 | Train Acc: 99.83% | Val Loss: 1.1931 | Val Acc: 73.49%\n",
      "Epoch 71/200 | Train Loss: 0.0183 | Train Acc: 99.83% | Val Loss: 1.1829 | Val Acc: 71.14%\n",
      "Epoch 72/200 | Train Loss: 0.0163 | Train Acc: 99.83% | Val Loss: 1.2089 | Val Acc: 72.15%\n",
      "Epoch 73/200 | Train Loss: 0.0155 | Train Acc: 99.83% | Val Loss: 1.2492 | Val Acc: 72.48%\n",
      "Epoch 74/200 | Train Loss: 0.0125 | Train Acc: 99.92% | Val Loss: 1.2164 | Val Acc: 72.48%\n",
      "Epoch 75/200 | Train Loss: 0.0113 | Train Acc: 99.92% | Val Loss: 1.2770 | Val Acc: 72.48%\n",
      "Epoch 76/200 | Train Loss: 0.0098 | Train Acc: 100.00% | Val Loss: 1.2459 | Val Acc: 71.81%\n",
      "Early stopping at epoch 75. Best val loss: 0.8003\n"
     ]
    }
   ],
   "source": [
    "best_model = train(model, train_loader,val_loader ,criterion, optimizer,scheduler, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.2372 | Test Accuracy: 71.56%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dla2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
