{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 43\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', str(text).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1490, Test samples: 735\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Datasets/TrainData.csv\")\n",
    "test_df = pd.read_csv(\"Datasets/TestLabels.csv\")\n",
    "\n",
    "train_df.dropna(subset=['Text', 'Category'], inplace=True)\n",
    "test_df.dropna(subset=['Text', 'Label - (business, tech, politics, sport, entertainment)'], inplace=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Test samples: {len(test_df)}\")\n",
    "\n",
    "train_texts = train_df['Text'].tolist()\n",
    "train_labels = train_df['Category'].tolist()\n",
    "test_texts = test_df['Text'].tolist()\n",
    "test_labels = test_df['Label - (business, tech, politics, sport, entertainment)'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = [simple_tokenizer(t) for t in train_texts]\n",
    "test_tokenized = [simple_tokenizer(t) for t in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(inner_array) for inner_array in train_tokenized]\n",
    "max_len = int(np.percentile(lengths, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, tokenized_text, labels, vocab=None, label2idx=None, max_len=300):\n",
    "        self.texts = tokenized_text\n",
    "        self.max_len = max_len\n",
    "        if vocab is None:\n",
    "            words = [word for text in self.texts for word in text]\n",
    "            word_freq = Counter(words)\n",
    "            self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "            for word in word_freq:\n",
    "                self.vocab[word] = len(self.vocab)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.texts = [self.encode(text) for text in self.texts]\n",
    "\n",
    "        if label2idx is None:\n",
    "            unique_labels = sorted(set(label for label in labels if pd.notna(label)))\n",
    "            self.label2idx = {label: i for i, label in enumerate(unique_labels)}\n",
    "        else:\n",
    "            self.label2idx = label2idx\n",
    "\n",
    "        self.labels = [self.label2idx[label] for label in labels if pd.notna(label)]\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        encoded = [self.vocab.get(tok, self.vocab['<UNK>']) for tok in tokens]\n",
    "        return encoded[:self.max_len] + [0]*(self.max_len - len(encoded))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NewsDataset(train_tokenized, train_labels)\n",
    "test_data = NewsDataset(test_tokenized, test_labels, vocab=train_data.vocab, label2idx=train_data.label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_classes=5,\n",
    "                 num_layers=4, num_heads=8, max_len=300, pos_embed=True):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.pos_embed_enabled = pos_embed\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        if pos_embed:\n",
    "            self.position_embedding = nn.Embedding(max_len, embed_dim)\n",
    "        \n",
    "        # Transformer encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        x_embed = self.word_embedding(x)\n",
    "        if self.pos_embed_enabled:\n",
    "            x_embed += self.position_embedding(positions)\n",
    "        \n",
    "        x_transformed = self.transformer_encoder(x_embed)\n",
    "        x_pooled = x_transformed.mean(dim=1)  # Mean pooling across sequence length\n",
    "        \n",
    "        return self.classifier(x_pooled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    all_preds ,all_labels = [] ,[]\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().tolist())\n",
    "            all_labels.extend(y_batch.cpu().tolist())\n",
    "\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    f1 = f1_score(all_labels,all_preds,average='micro')\n",
    "    return avg_loss, accuracy,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for texts, labels in loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    return avg_train_loss , train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "lr = 0.0003\n",
    "weight_decay = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kashp\\miniconda3\\envs\\dla3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    \"vocab_size\": len(train_data.vocab),\n",
    "    \"embed_dim\": 512,\n",
    "    \"num_classes\": len(train_data.label2idx),\n",
    "    \"num_layers\": 2,\n",
    "    \"num_heads\": 8,\n",
    "    \"max_len\": max_len,\n",
    "    \"pos_embed\": True\n",
    "}\n",
    "\n",
    "model = TextTransformer(**model_params).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0002)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 1.5849 | Train Acc: 23.29% | Val Loss: 1.5235 | Val Acc: 28.30% | f1: 0.28\n",
      "Epoch 2/50 | Train Loss: 1.5151 | Train Acc: 29.40% | Val Loss: 1.4880 | Val Acc: 36.87% | f1: 0.37\n",
      "Epoch 3/50 | Train Loss: 1.3785 | Train Acc: 40.67% | Val Loss: 1.3682 | Val Acc: 36.87% | f1: 0.37\n",
      "Epoch 4/50 | Train Loss: 1.2530 | Train Acc: 46.38% | Val Loss: 1.2676 | Val Acc: 42.31% | f1: 0.42\n",
      "Epoch 5/50 | Train Loss: 1.1642 | Train Acc: 52.08% | Val Loss: 1.2061 | Val Acc: 51.43% | f1: 0.51\n",
      "Epoch 6/50 | Train Loss: 1.0562 | Train Acc: 57.58% | Val Loss: 1.0619 | Val Acc: 56.73% | f1: 0.57\n",
      "Epoch 7/50 | Train Loss: 0.9416 | Train Acc: 64.70% | Val Loss: 0.9561 | Val Acc: 67.07% | f1: 0.67\n",
      "Epoch 8/50 | Train Loss: 0.8567 | Train Acc: 69.33% | Val Loss: 0.8898 | Val Acc: 70.34% | f1: 0.70\n",
      "Epoch 9/50 | Train Loss: 0.7646 | Train Acc: 75.97% | Val Loss: 0.8264 | Val Acc: 77.28% | f1: 0.77\n",
      "Epoch 10/50 | Train Loss: 0.6465 | Train Acc: 83.62% | Val Loss: 0.6977 | Val Acc: 82.45% | f1: 0.82\n",
      "Epoch 11/50 | Train Loss: 0.5296 | Train Acc: 89.26% | Val Loss: 0.6713 | Val Acc: 76.05% | f1: 0.76\n",
      "Epoch 12/50 | Train Loss: 0.4397 | Train Acc: 91.34% | Val Loss: 0.5681 | Val Acc: 84.63% | f1: 0.85\n",
      "Epoch 13/50 | Train Loss: 0.3415 | Train Acc: 95.70% | Val Loss: 0.4894 | Val Acc: 87.89% | f1: 0.88\n",
      "Epoch 14/50 | Train Loss: 0.2863 | Train Acc: 96.44% | Val Loss: 0.4719 | Val Acc: 87.21% | f1: 0.87\n",
      "Epoch 15/50 | Train Loss: 0.2220 | Train Acc: 97.72% | Val Loss: 0.4446 | Val Acc: 87.48% | f1: 0.87\n",
      "Epoch 16/50 | Train Loss: 0.1879 | Train Acc: 98.93% | Val Loss: 0.3784 | Val Acc: 91.02% | f1: 0.91\n",
      "Epoch 17/50 | Train Loss: 0.1668 | Train Acc: 99.40% | Val Loss: 0.3880 | Val Acc: 89.66% | f1: 0.90\n",
      "Epoch 18/50 | Train Loss: 0.1378 | Train Acc: 99.06% | Val Loss: 0.3732 | Val Acc: 90.48% | f1: 0.90\n",
      "Epoch 19/50 | Train Loss: 0.1066 | Train Acc: 99.73% | Val Loss: 0.3521 | Val Acc: 90.61% | f1: 0.91\n",
      "Epoch 20/50 | Train Loss: 0.0869 | Train Acc: 99.80% | Val Loss: 0.3389 | Val Acc: 90.75% | f1: 0.91\n",
      "Epoch 21/50 | Train Loss: 0.0778 | Train Acc: 99.87% | Val Loss: 0.3378 | Val Acc: 92.11% | f1: 0.92\n",
      "Epoch 22/50 | Train Loss: 0.0696 | Train Acc: 99.73% | Val Loss: 0.3076 | Val Acc: 92.79% | f1: 0.93\n",
      "Epoch 23/50 | Train Loss: 0.0548 | Train Acc: 99.87% | Val Loss: 0.3054 | Val Acc: 91.84% | f1: 0.92\n",
      "Epoch 24/50 | Train Loss: 0.0489 | Train Acc: 99.87% | Val Loss: 0.3038 | Val Acc: 91.70% | f1: 0.92\n",
      "Epoch 25/50 | Train Loss: 0.0455 | Train Acc: 99.93% | Val Loss: 0.3044 | Val Acc: 91.29% | f1: 0.91\n",
      "Epoch 26/50 | Train Loss: 0.0356 | Train Acc: 99.93% | Val Loss: 0.2992 | Val Acc: 92.52% | f1: 0.93\n",
      "Epoch 27/50 | Train Loss: 0.0307 | Train Acc: 99.93% | Val Loss: 0.2690 | Val Acc: 93.20% | f1: 0.93\n",
      "Epoch 28/50 | Train Loss: 0.0291 | Train Acc: 100.00% | Val Loss: 0.2859 | Val Acc: 92.52% | f1: 0.93\n",
      "Epoch 29/50 | Train Loss: 0.0266 | Train Acc: 99.93% | Val Loss: 0.2586 | Val Acc: 93.61% | f1: 0.94\n",
      "Epoch 30/50 | Train Loss: 0.0212 | Train Acc: 100.00% | Val Loss: 0.2597 | Val Acc: 93.61% | f1: 0.94\n",
      "Epoch 31/50 | Train Loss: 0.0217 | Train Acc: 100.00% | Val Loss: 0.2736 | Val Acc: 92.24% | f1: 0.92\n"
     ]
    }
   ],
   "source": [
    "patience = 15 \n",
    "best_val_loss = float('inf')\n",
    "counter = 0  \n",
    "for epoch in range(EPOCHS):\n",
    "    avg_train_loss , train_accuracy = train_model(model, train_loader, criterion, optimizer)\n",
    "    avg_val_loss, val_accuracy ,f1 = evaluate_model(model, test_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}% | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | f1: {f1:.2f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0 \n",
    "        best_model = model.state_dict()  \n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}. Best val loss: {best_val_loss:.4f}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Micro F1 Score = 0.9429\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation and confusion matrix\n",
    "avg_val_loss, val_accuracy, f1 = evaluate_model(model, test_loader, criterion)\n",
    "print(f\"\\nFinal Micro F1 Score = {f1:.4f}\")\n",
    "label_names = [label for label, _ in sorted(train_data.label2idx.items(), key=lambda x: x[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dla3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
