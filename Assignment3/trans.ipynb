{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 43\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', str(text).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1490, Test samples: 735\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Datasets/TrainData.csv\")\n",
    "test_df = pd.read_csv(\"Datasets/TestLabels.csv\")\n",
    "\n",
    "train_df.dropna(subset=['Text', 'Category'], inplace=True)\n",
    "test_df.dropna(subset=['Text', 'Label - (business, tech, politics, sport, entertainment)'], inplace=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Test samples: {len(test_df)}\")\n",
    "\n",
    "train_texts = train_df['Text'].tolist()\n",
    "train_labels = train_df['Category'].tolist()\n",
    "test_texts = test_df['Text'].tolist()\n",
    "test_labels = test_df['Label - (business, tech, politics, sport, entertainment)'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = [simple_tokenizer(t) for t in train_texts]\n",
    "test_tokenized = [simple_tokenizer(t) for t in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(inner_array) for inner_array in train_tokenized]\n",
    "max_len = int(np.percentile(lengths, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, tokenized_text, labels, vocab=None, label2idx=None, max_len=300):\n",
    "        self.texts = tokenized_text\n",
    "        self.max_len = max_len\n",
    "        if vocab is None:\n",
    "            words = [word for text in self.texts for word in text]\n",
    "            word_freq = Counter(words)\n",
    "            self.vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "            for word in word_freq:\n",
    "                self.vocab[word] = len(self.vocab)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.texts = [self.encode(text) for text in self.texts]\n",
    "\n",
    "        if label2idx is None:\n",
    "            unique_labels = sorted(set(label for label in labels if pd.notna(label)))\n",
    "            self.label2idx = {label: i for i, label in enumerate(unique_labels)}\n",
    "        else:\n",
    "            self.label2idx = label2idx\n",
    "\n",
    "        self.labels = [self.label2idx[label] for label in labels if pd.notna(label)]\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        encoded = [self.vocab.get(tok, self.vocab['<UNK>']) for tok in tokens]\n",
    "        return encoded[:self.max_len] + [0]*(self.max_len - len(encoded))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NewsDataset(train_tokenized, train_labels)\n",
    "test_data = NewsDataset(test_tokenized, test_labels, vocab=train_data.vocab, label2idx=train_data.label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_classes=5,\n",
    "                 num_layers=4, num_heads=8, max_len=300, pos_embed=True):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.pos_embed_enabled = pos_embed\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        if pos_embed:\n",
    "            self.position_embedding = nn.Embedding(max_len, embed_dim)\n",
    "        \n",
    "        # Transformer encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        x_embed = self.word_embedding(x)\n",
    "        if self.pos_embed_enabled:\n",
    "            x_embed += self.position_embedding(positions)\n",
    "        \n",
    "        x_transformed = self.transformer_encoder(x_embed)\n",
    "        x_pooled = x_transformed.mean(dim=1)  # Mean pooling across sequence length\n",
    "        \n",
    "        return self.classifier(x_pooled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    all_preds ,all_labels = [] ,[]\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().tolist())\n",
    "            all_labels.extend(y_batch.cpu().tolist())\n",
    "\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    f1 = f1_score(all_labels,all_preds,average='micro')\n",
    "    return avg_loss, accuracy,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for texts, labels in loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    return avg_train_loss , train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "lr = 0.0003\n",
    "weight_decay = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kashp\\miniconda3\\envs\\dla3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    \"vocab_size\": len(train_data.vocab),\n",
    "    \"embed_dim\": 512,\n",
    "    \"num_classes\": len(train_data.label2idx),\n",
    "    \"num_layers\": 1,\n",
    "    \"num_heads\": 8,\n",
    "    \"max_len\": max_len,\n",
    "    \"pos_embed\": True\n",
    "}\n",
    "\n",
    "model = TextTransformer(**model_params).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0002)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Analyzing Encoder Blocks...\n",
      "\n",
      "=== Training with 2 encoder blocks ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kashp\\miniconda3\\envs\\dla3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 1.5667 | Val F1: 0.2844\n",
      "Epoch 2/50 | Train Loss: 1.4945 | Val F1: 0.3633\n",
      "Epoch 3/50 | Train Loss: 1.3243 | Val F1: 0.4599\n",
      "Epoch 4/50 | Train Loss: 1.0738 | Val F1: 0.6993\n",
      "Epoch 5/50 | Train Loss: 0.8217 | Val F1: 0.7143\n",
      "Epoch 6/50 | Train Loss: 0.6669 | Val F1: 0.7252\n",
      "Epoch 7/50 | Train Loss: 0.5451 | Val F1: 0.8435\n",
      "Epoch 8/50 | Train Loss: 0.4401 | Val F1: 0.8095\n",
      "Epoch 9/50 | Train Loss: 0.3662 | Val F1: 0.8544\n",
      "Epoch 10/50 | Train Loss: 0.2934 | Val F1: 0.8503\n",
      "Epoch 11/50 | Train Loss: 0.2503 | Val F1: 0.8694\n",
      "Epoch 12/50 | Train Loss: 0.2134 | Val F1: 0.8558\n",
      "Epoch 13/50 | Train Loss: 0.1684 | Val F1: 0.8844\n",
      "Epoch 14/50 | Train Loss: 0.1354 | Val F1: 0.9102\n",
      "Epoch 15/50 | Train Loss: 0.1116 | Val F1: 0.9007\n",
      "Epoch 16/50 | Train Loss: 0.0956 | Val F1: 0.9061\n",
      "Epoch 17/50 | Train Loss: 0.0861 | Val F1: 0.9007\n",
      "Epoch 18/50 | Train Loss: 0.0750 | Val F1: 0.9075\n",
      "Epoch 19/50 | Train Loss: 0.0609 | Val F1: 0.8952\n",
      "Epoch 20/50 | Train Loss: 0.0524 | Val F1: 0.9143\n",
      "Epoch 21/50 | Train Loss: 0.0428 | Val F1: 0.9197\n",
      "Epoch 22/50 | Train Loss: 0.0396 | Val F1: 0.9252\n",
      "Epoch 23/50 | Train Loss: 0.0369 | Val F1: 0.9116\n",
      "Epoch 24/50 | Train Loss: 0.0330 | Val F1: 0.9265\n",
      "Epoch 25/50 | Train Loss: 0.0286 | Val F1: 0.9184\n",
      "Epoch 26/50 | Train Loss: 0.0259 | Val F1: 0.9170\n",
      "Epoch 27/50 | Train Loss: 0.0204 | Val F1: 0.9061\n",
      "Epoch 28/50 | Train Loss: 0.0212 | Val F1: 0.9252\n",
      "Epoch 29/50 | Train Loss: 0.0222 | Val F1: 0.9224\n",
      "Epoch 30/50 | Train Loss: 0.0180 | Val F1: 0.9252\n",
      "Epoch 31/50 | Train Loss: 0.0168 | Val F1: 0.9184\n",
      "Epoch 32/50 | Train Loss: 0.0151 | Val F1: 0.9170\n",
      "Epoch 33/50 | Train Loss: 0.0134 | Val F1: 0.9211\n",
      "Epoch 34/50 | Train Loss: 0.0124 | Val F1: 0.9156\n",
      "Epoch 35/50 | Train Loss: 0.0107 | Val F1: 0.9265\n",
      "Epoch 36/50 | Train Loss: 0.0096 | Val F1: 0.9238\n",
      "Epoch 37/50 | Train Loss: 0.0088 | Val F1: 0.9238\n",
      "Epoch 38/50 | Train Loss: 0.0106 | Val F1: 0.9197\n",
      "Epoch 39/50 | Train Loss: 0.0120 | Val F1: 0.9197\n",
      "\n",
      "=== Training with 4 encoder blocks ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kashp\\miniconda3\\envs\\dla3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 1.5818 | Val F1: 0.2803\n",
      "Epoch 2/50 | Train Loss: 1.4821 | Val F1: 0.4367\n",
      "Epoch 3/50 | Train Loss: 1.3442 | Val F1: 0.4626\n",
      "Epoch 4/50 | Train Loss: 1.1034 | Val F1: 0.6803\n",
      "Epoch 5/50 | Train Loss: 0.9054 | Val F1: 0.7224\n",
      "Epoch 6/50 | Train Loss: 0.7078 | Val F1: 0.7946\n",
      "Epoch 7/50 | Train Loss: 0.5783 | Val F1: 0.8041\n",
      "Epoch 8/50 | Train Loss: 0.4521 | Val F1: 0.8612\n",
      "Epoch 9/50 | Train Loss: 0.3507 | Val F1: 0.8558\n",
      "Epoch 10/50 | Train Loss: 0.2869 | Val F1: 0.8639\n",
      "Epoch 11/50 | Train Loss: 0.2271 | Val F1: 0.8762\n",
      "Epoch 12/50 | Train Loss: 0.1966 | Val F1: 0.8571\n",
      "Epoch 13/50 | Train Loss: 0.1592 | Val F1: 0.8952\n",
      "Epoch 14/50 | Train Loss: 0.1300 | Val F1: 0.8748\n",
      "Epoch 15/50 | Train Loss: 0.1015 | Val F1: 0.8925\n",
      "Epoch 16/50 | Train Loss: 0.0876 | Val F1: 0.9102\n",
      "Epoch 17/50 | Train Loss: 0.0725 | Val F1: 0.8980\n",
      "Epoch 18/50 | Train Loss: 0.0604 | Val F1: 0.9102\n",
      "Epoch 19/50 | Train Loss: 0.0465 | Val F1: 0.9293\n",
      "Epoch 20/50 | Train Loss: 0.0429 | Val F1: 0.9170\n",
      "Epoch 21/50 | Train Loss: 0.0382 | Val F1: 0.9048\n",
      "Epoch 22/50 | Train Loss: 0.0332 | Val F1: 0.9252\n",
      "Epoch 23/50 | Train Loss: 0.0285 | Val F1: 0.9129\n",
      "Epoch 24/50 | Train Loss: 0.0248 | Val F1: 0.9184\n",
      "Epoch 25/50 | Train Loss: 0.0230 | Val F1: 0.9184\n",
      "Epoch 26/50 | Train Loss: 0.0207 | Val F1: 0.9156\n",
      "Epoch 27/50 | Train Loss: 0.0184 | Val F1: 0.9143\n",
      "Epoch 28/50 | Train Loss: 0.0214 | Val F1: 0.9184\n",
      "Epoch 29/50 | Train Loss: 0.0150 | Val F1: 0.9279\n",
      "Epoch 30/50 | Train Loss: 0.0146 | Val F1: 0.9265\n",
      "Epoch 31/50 | Train Loss: 0.0120 | Val F1: 0.9129\n",
      "Epoch 32/50 | Train Loss: 0.0108 | Val F1: 0.9211\n",
      "Epoch 33/50 | Train Loss: 0.0099 | Val F1: 0.9265\n",
      "Epoch 34/50 | Train Loss: 0.0096 | Val F1: 0.9129\n",
      "\n",
      "=== Training with 6 encoder blocks ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kashp\\miniconda3\\envs\\dla3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 1.5821 | Val F1: 0.2857\n",
      "Epoch 2/50 | Train Loss: 1.5027 | Val F1: 0.3959\n",
      "Epoch 3/50 | Train Loss: 1.3321 | Val F1: 0.4000\n",
      "Epoch 4/50 | Train Loss: 1.2277 | Val F1: 0.4327\n",
      "Epoch 5/50 | Train Loss: 1.0694 | Val F1: 0.5007\n",
      "Epoch 6/50 | Train Loss: 0.9389 | Val F1: 0.6748\n",
      "Epoch 7/50 | Train Loss: 0.8280 | Val F1: 0.7483\n",
      "Epoch 8/50 | Train Loss: 0.6806 | Val F1: 0.8218\n",
      "Epoch 9/50 | Train Loss: 0.5509 | Val F1: 0.8177\n",
      "Epoch 10/50 | Train Loss: 0.4622 | Val F1: 0.7483\n",
      "Epoch 11/50 | Train Loss: 0.4006 | Val F1: 0.8639\n",
      "Epoch 12/50 | Train Loss: 0.3069 | Val F1: 0.8395\n",
      "Epoch 13/50 | Train Loss: 0.2545 | Val F1: 0.8721\n",
      "Epoch 14/50 | Train Loss: 0.2207 | Val F1: 0.9007\n",
      "Epoch 15/50 | Train Loss: 0.1781 | Val F1: 0.8884\n",
      "Epoch 16/50 | Train Loss: 0.1571 | Val F1: 0.9088\n",
      "Epoch 17/50 | Train Loss: 0.1340 | Val F1: 0.8571\n",
      "Epoch 18/50 | Train Loss: 0.1098 | Val F1: 0.9075\n",
      "Epoch 19/50 | Train Loss: 0.0816 | Val F1: 0.9224\n",
      "Epoch 20/50 | Train Loss: 0.0607 | Val F1: 0.9170\n",
      "Epoch 21/50 | Train Loss: 0.0592 | Val F1: 0.9075\n",
      "Epoch 22/50 | Train Loss: 0.0516 | Val F1: 0.9238\n",
      "Epoch 23/50 | Train Loss: 0.0441 | Val F1: 0.9143\n",
      "Epoch 24/50 | Train Loss: 0.0421 | Val F1: 0.9211\n",
      "Epoch 25/50 | Train Loss: 0.0383 | Val F1: 0.9333\n",
      "Epoch 26/50 | Train Loss: 0.0315 | Val F1: 0.9224\n",
      "Epoch 27/50 | Train Loss: 0.0279 | Val F1: 0.9238\n",
      "Epoch 28/50 | Train Loss: 0.0215 | Val F1: 0.9224\n",
      "Epoch 29/50 | Train Loss: 0.0187 | Val F1: 0.9293\n",
      "Epoch 30/50 | Train Loss: 0.0175 | Val F1: 0.9238\n",
      "Epoch 31/50 | Train Loss: 0.0157 | Val F1: 0.9279\n",
      "Epoch 32/50 | Train Loss: 0.0145 | Val F1: 0.9252\n",
      "Epoch 33/50 | Train Loss: 0.0137 | Val F1: 0.9170\n",
      "Epoch 34/50 | Train Loss: 0.0119 | Val F1: 0.9265\n",
      "Epoch 35/50 | Train Loss: 0.0112 | Val F1: 0.9156\n",
      "Epoch 36/50 | Train Loss: 0.0128 | Val F1: 0.9320\n",
      "Epoch 37/50 | Train Loss: 0.0142 | Val F1: 0.9252\n",
      "Epoch 38/50 | Train Loss: 0.0106 | Val F1: 0.9279\n",
      "Epoch 39/50 | Train Loss: 0.0091 | Val F1: 0.9252\n",
      "Epoch 40/50 | Train Loss: 0.0083 | Val F1: 0.9252\n",
      "\n",
      "Encoder Block Results:\n",
      "2 layers: F1 = 0.9265\n",
      "4 layers: F1 = 0.9293\n",
      "6 layers: F1 = 0.9333\n",
      "\n",
      "==================================================\n",
      "Analyzing Positional Embeddings...\n",
      "\n",
      "=== Training with positional embeddings ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kashp\\miniconda3\\envs\\dla3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 1.5547 | Val F1: 0.2490\n",
      "Epoch 2/50 | Train Loss: 1.4691 | Val F1: 0.4218\n",
      "Epoch 3/50 | Train Loss: 1.3224 | Val F1: 0.3429\n",
      "Epoch 4/50 | Train Loss: 1.1459 | Val F1: 0.6517\n",
      "Epoch 5/50 | Train Loss: 0.9198 | Val F1: 0.6612\n",
      "Epoch 6/50 | Train Loss: 0.7057 | Val F1: 0.7946\n",
      "Epoch 7/50 | Train Loss: 0.5279 | Val F1: 0.8585\n",
      "Epoch 8/50 | Train Loss: 0.4115 | Val F1: 0.8694\n",
      "Epoch 9/50 | Train Loss: 0.3100 | Val F1: 0.8816\n",
      "Epoch 10/50 | Train Loss: 0.2519 | Val F1: 0.8639\n",
      "Epoch 11/50 | Train Loss: 0.2081 | Val F1: 0.8571\n",
      "Epoch 12/50 | Train Loss: 0.1762 | Val F1: 0.9088\n",
      "Epoch 13/50 | Train Loss: 0.1411 | Val F1: 0.9088\n",
      "Epoch 14/50 | Train Loss: 0.1176 | Val F1: 0.8816\n",
      "Epoch 15/50 | Train Loss: 0.0998 | Val F1: 0.8871\n",
      "Epoch 16/50 | Train Loss: 0.0832 | Val F1: 0.9156\n",
      "Epoch 17/50 | Train Loss: 0.0719 | Val F1: 0.8884\n",
      "Epoch 18/50 | Train Loss: 0.0580 | Val F1: 0.9184\n",
      "Epoch 19/50 | Train Loss: 0.0505 | Val F1: 0.9102\n",
      "Epoch 20/50 | Train Loss: 0.0438 | Val F1: 0.9184\n",
      "Epoch 21/50 | Train Loss: 0.0395 | Val F1: 0.8803\n",
      "Epoch 22/50 | Train Loss: 0.0383 | Val F1: 0.9102\n",
      "Epoch 23/50 | Train Loss: 0.0369 | Val F1: 0.8993\n",
      "Epoch 24/50 | Train Loss: 0.0317 | Val F1: 0.9197\n",
      "Epoch 25/50 | Train Loss: 0.0274 | Val F1: 0.9048\n",
      "Epoch 26/50 | Train Loss: 0.0242 | Val F1: 0.8993\n",
      "Epoch 27/50 | Train Loss: 0.0214 | Val F1: 0.9170\n",
      "Epoch 28/50 | Train Loss: 0.0204 | Val F1: 0.8993\n",
      "Epoch 29/50 | Train Loss: 0.0215 | Val F1: 0.9116\n",
      "Epoch 30/50 | Train Loss: 0.0184 | Val F1: 0.9143\n",
      "Epoch 31/50 | Train Loss: 0.0159 | Val F1: 0.9211\n",
      "Epoch 32/50 | Train Loss: 0.0166 | Val F1: 0.8980\n",
      "Epoch 33/50 | Train Loss: 0.0117 | Val F1: 0.9293\n",
      "Epoch 34/50 | Train Loss: 0.0114 | Val F1: 0.9293\n",
      "Epoch 35/50 | Train Loss: 0.0118 | Val F1: 0.9279\n",
      "Epoch 36/50 | Train Loss: 0.0160 | Val F1: 0.9034\n",
      "Epoch 37/50 | Train Loss: 0.0150 | Val F1: 0.9184\n",
      "Epoch 38/50 | Train Loss: 0.0101 | Val F1: 0.9061\n",
      "Epoch 39/50 | Train Loss: 0.0082 | Val F1: 0.9197\n",
      "Epoch 40/50 | Train Loss: 0.0078 | Val F1: 0.9252\n",
      "Epoch 41/50 | Train Loss: 0.0077 | Val F1: 0.9279\n",
      "Epoch 42/50 | Train Loss: 0.0069 | Val F1: 0.9279\n",
      "Epoch 43/50 | Train Loss: 0.0066 | Val F1: 0.9129\n",
      "Epoch 44/50 | Train Loss: 0.0071 | Val F1: 0.9061\n",
      "Epoch 45/50 | Train Loss: 0.0067 | Val F1: 0.9265\n",
      "Epoch 46/50 | Train Loss: 0.0073 | Val F1: 0.9306\n",
      "Epoch 47/50 | Train Loss: 0.0085 | Val F1: 0.9415\n",
      "Epoch 48/50 | Train Loss: 0.0066 | Val F1: 0.9442\n",
      "Epoch 49/50 | Train Loss: 0.0053 | Val F1: 0.9224\n",
      "Epoch 50/50 | Train Loss: 0.0049 | Val F1: 0.9211\n",
      "\n",
      "=== Training without positional embeddings ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kashp\\miniconda3\\envs\\dla3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 1.5310 | Val F1: 0.4735\n",
      "Epoch 2/50 | Train Loss: 1.3746 | Val F1: 0.4707\n",
      "Epoch 3/50 | Train Loss: 1.0452 | Val F1: 0.7333\n",
      "Epoch 4/50 | Train Loss: 0.6977 | Val F1: 0.8272\n",
      "Epoch 5/50 | Train Loss: 0.4838 | Val F1: 0.8340\n",
      "Epoch 6/50 | Train Loss: 0.3825 | Val F1: 0.8857\n",
      "Epoch 7/50 | Train Loss: 0.2967 | Val F1: 0.9075\n",
      "Epoch 8/50 | Train Loss: 0.2390 | Val F1: 0.9088\n",
      "Epoch 9/50 | Train Loss: 0.1932 | Val F1: 0.9388\n",
      "Epoch 10/50 | Train Loss: 0.1561 | Val F1: 0.9265\n",
      "Epoch 11/50 | Train Loss: 0.1275 | Val F1: 0.9333\n",
      "Epoch 12/50 | Train Loss: 0.1155 | Val F1: 0.9374\n",
      "Epoch 13/50 | Train Loss: 0.0978 | Val F1: 0.9333\n",
      "Epoch 14/50 | Train Loss: 0.0872 | Val F1: 0.9293\n",
      "Epoch 15/50 | Train Loss: 0.0757 | Val F1: 0.9401\n",
      "Epoch 16/50 | Train Loss: 0.0641 | Val F1: 0.9442\n",
      "Epoch 17/50 | Train Loss: 0.0536 | Val F1: 0.9469\n",
      "Epoch 18/50 | Train Loss: 0.0488 | Val F1: 0.9401\n",
      "Epoch 19/50 | Train Loss: 0.0418 | Val F1: 0.9401\n",
      "Epoch 20/50 | Train Loss: 0.0421 | Val F1: 0.9415\n",
      "Epoch 21/50 | Train Loss: 0.0399 | Val F1: 0.9415\n",
      "Epoch 22/50 | Train Loss: 0.0339 | Val F1: 0.9469\n",
      "Epoch 23/50 | Train Loss: 0.0277 | Val F1: 0.9483\n",
      "Epoch 24/50 | Train Loss: 0.0283 | Val F1: 0.9456\n",
      "Epoch 25/50 | Train Loss: 0.0319 | Val F1: 0.9374\n",
      "Epoch 26/50 | Train Loss: 0.0240 | Val F1: 0.9388\n",
      "Epoch 27/50 | Train Loss: 0.0206 | Val F1: 0.9429\n",
      "Epoch 28/50 | Train Loss: 0.0189 | Val F1: 0.9429\n",
      "Epoch 29/50 | Train Loss: 0.0171 | Val F1: 0.9415\n",
      "Epoch 30/50 | Train Loss: 0.0156 | Val F1: 0.9442\n",
      "Epoch 31/50 | Train Loss: 0.0139 | Val F1: 0.9415\n",
      "Epoch 32/50 | Train Loss: 0.0120 | Val F1: 0.9429\n",
      "Epoch 33/50 | Train Loss: 0.0120 | Val F1: 0.9388\n",
      "Epoch 34/50 | Train Loss: 0.0109 | Val F1: 0.9429\n",
      "Epoch 35/50 | Train Loss: 0.0104 | Val F1: 0.9429\n",
      "Epoch 36/50 | Train Loss: 0.0105 | Val F1: 0.9429\n",
      "Epoch 37/50 | Train Loss: 0.0095 | Val F1: 0.9456\n",
      "Epoch 38/50 | Train Loss: 0.0086 | Val F1: 0.9388\n",
      "\n",
      "Positional Embedding Results:\n",
      "With Pos: F1 = 0.9442\n",
      "Without Pos: F1 = 0.9483\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "patience = 15 \n",
    "def analyze_encoder_blocks():\n",
    "    \"\"\"Analyze performance with different numbers of encoder blocks\"\"\"\n",
    "    encoder_results = {}\n",
    "    \n",
    "    for num_layers in [2, 4, 6]:\n",
    "        print(f\"\\n=== Training with {num_layers} encoder blocks ===\")\n",
    "        \n",
    "        # Model configuration\n",
    "        model_params = {\n",
    "            \"vocab_size\": len(train_data.vocab),\n",
    "            \"embed_dim\": 512,\n",
    "            \"num_classes\": len(train_data.label2idx),\n",
    "            \"num_layers\": num_layers,\n",
    "            \"num_heads\": 8,\n",
    "            \"max_len\": max_len,\n",
    "            \"pos_embed\": True\n",
    "        }\n",
    "        \n",
    "        # Initialize fresh model\n",
    "        model = TextTransformer(**model_params).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.0002)\n",
    "        \n",
    "        best_f1 = 0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            avg_train_loss, train_acc = train_model(model, train_loader, criterion, optimizer)\n",
    "            avg_val_loss, val_acc, val_f1 = evaluate_model(model, test_loader, criterion)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} | Val F1: {val_f1:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "                \n",
    "        encoder_results[num_layers] = best_f1\n",
    "    \n",
    "    return encoder_results\n",
    "\n",
    "def analyze_positional_embeddings():\n",
    "    \"\"\"Analyze performance with/without positional embeddings\"\"\"\n",
    "    pos_results = {}\n",
    "    \n",
    "    for use_pos in [True, False]:\n",
    "        print(f\"\\n=== Training {'with' if use_pos else 'without'} positional embeddings ===\")\n",
    "        \n",
    "        # Model configuration\n",
    "        model_params = {\n",
    "            \"vocab_size\": len(train_data.vocab),\n",
    "            \"embed_dim\": 512,\n",
    "            \"num_classes\": len(train_data.label2idx),\n",
    "            \"num_layers\": 2,  # Default layer count\n",
    "            \"num_heads\": 8,\n",
    "            \"max_len\": max_len,\n",
    "            \"pos_embed\": use_pos\n",
    "        }\n",
    "        \n",
    "        # Initialize fresh model\n",
    "        model = TextTransformer(**model_params).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.0002)\n",
    "        \n",
    "        best_f1 = 0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            avg_train_loss, train_acc = train_model(model, train_loader, criterion, optimizer)\n",
    "            avg_val_loss, val_acc, val_f1 = evaluate_model(model, test_loader, criterion)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} | Val F1: {val_f1:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "                \n",
    "        pos_results['With Pos' if use_pos else 'Without Pos'] = best_f1\n",
    "    \n",
    "    return pos_results\n",
    "\n",
    "# Add this visualization function\n",
    "def plot_results(results, title, xlabel):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(results.keys(), results.values())\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Micro F1 Score\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "# Run analyses after your single model training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Analyzing Encoder Blocks...\")\n",
    "encoder_results = analyze_encoder_blocks()\n",
    "print(\"\\nEncoder Block Results:\")\n",
    "for layers, f1 in encoder_results.items():\n",
    "    print(f\"{layers} layers: F1 = {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Analyzing Positional Embeddings...\")\n",
    "pos_results = analyze_positional_embeddings()\n",
    "print(\"\\nPositional Embedding Results:\")\n",
    "for config, f1 in pos_results.items():\n",
    "    print(f\"{config}: F1 = {f1:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plot_results(encoder_results, \"Effect of Encoder Blocks\", \"Number of Encoder Layers\")\n",
    "plot_results(pos_results, \"Effect of Positional Embeddings\", \"Configuration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 1.5530 | Train Acc: 26.78% | Val Loss: 1.4968 | Val Acc: 29.66% | f1: 0.30\n",
      "Epoch 2/50 | Train Loss: 1.4887 | Train Acc: 34.43% | Val Loss: 1.4190 | Val Acc: 50.88% | f1: 0.51\n",
      "Epoch 3/50 | Train Loss: 1.3480 | Train Acc: 44.43% | Val Loss: 1.2516 | Val Acc: 51.97% | f1: 0.52\n",
      "Epoch 4/50 | Train Loss: 1.1645 | Train Acc: 58.26% | Val Loss: 1.0561 | Val Acc: 67.35% | f1: 0.67\n",
      "Epoch 5/50 | Train Loss: 0.9214 | Train Acc: 72.48% | Val Loss: 0.8455 | Val Acc: 76.87% | f1: 0.77\n",
      "Epoch 6/50 | Train Loss: 0.7367 | Train Acc: 80.74% | Val Loss: 0.7231 | Val Acc: 82.86% | f1: 0.83\n",
      "Epoch 7/50 | Train Loss: 0.5636 | Train Acc: 90.67% | Val Loss: 0.6065 | Val Acc: 86.26% | f1: 0.86\n",
      "Epoch 8/50 | Train Loss: 0.4427 | Train Acc: 93.09% | Val Loss: 0.5331 | Val Acc: 88.44% | f1: 0.88\n",
      "Epoch 9/50 | Train Loss: 0.3702 | Train Acc: 93.83% | Val Loss: 0.4828 | Val Acc: 88.57% | f1: 0.89\n",
      "Epoch 10/50 | Train Loss: 0.2868 | Train Acc: 97.32% | Val Loss: 0.4956 | Val Acc: 84.49% | f1: 0.84\n",
      "Epoch 11/50 | Train Loss: 0.2371 | Train Acc: 97.32% | Val Loss: 0.4098 | Val Acc: 90.48% | f1: 0.90\n",
      "Epoch 12/50 | Train Loss: 0.1928 | Train Acc: 98.32% | Val Loss: 0.3789 | Val Acc: 90.61% | f1: 0.91\n",
      "Epoch 13/50 | Train Loss: 0.1665 | Train Acc: 98.86% | Val Loss: 0.3550 | Val Acc: 91.16% | f1: 0.91\n",
      "Epoch 14/50 | Train Loss: 0.1357 | Train Acc: 99.33% | Val Loss: 0.3568 | Val Acc: 90.34% | f1: 0.90\n",
      "Epoch 15/50 | Train Loss: 0.1106 | Train Acc: 99.46% | Val Loss: 0.3276 | Val Acc: 92.11% | f1: 0.92\n",
      "Epoch 16/50 | Train Loss: 0.0941 | Train Acc: 99.66% | Val Loss: 0.3542 | Val Acc: 89.39% | f1: 0.89\n",
      "Epoch 17/50 | Train Loss: 0.0821 | Train Acc: 99.80% | Val Loss: 0.3263 | Val Acc: 91.02% | f1: 0.91\n",
      "Epoch 18/50 | Train Loss: 0.0696 | Train Acc: 99.66% | Val Loss: 0.3401 | Val Acc: 90.07% | f1: 0.90\n",
      "Epoch 19/50 | Train Loss: 0.0594 | Train Acc: 99.93% | Val Loss: 0.3014 | Val Acc: 91.43% | f1: 0.91\n",
      "Epoch 20/50 | Train Loss: 0.0520 | Train Acc: 99.87% | Val Loss: 0.2816 | Val Acc: 93.47% | f1: 0.93\n",
      "Epoch 21/50 | Train Loss: 0.0466 | Train Acc: 99.87% | Val Loss: 0.2907 | Val Acc: 92.11% | f1: 0.92\n",
      "Epoch 22/50 | Train Loss: 0.0436 | Train Acc: 99.87% | Val Loss: 0.2661 | Val Acc: 93.20% | f1: 0.93\n",
      "Epoch 23/50 | Train Loss: 0.0352 | Train Acc: 99.93% | Val Loss: 0.2797 | Val Acc: 92.52% | f1: 0.93\n",
      "Epoch 24/50 | Train Loss: 0.0308 | Train Acc: 100.00% | Val Loss: 0.2761 | Val Acc: 92.38% | f1: 0.92\n",
      "Epoch 25/50 | Train Loss: 0.0288 | Train Acc: 100.00% | Val Loss: 0.2650 | Val Acc: 93.61% | f1: 0.94\n",
      "Epoch 26/50 | Train Loss: 0.0254 | Train Acc: 100.00% | Val Loss: 0.2598 | Val Acc: 93.20% | f1: 0.93\n",
      "Epoch 27/50 | Train Loss: 0.0243 | Train Acc: 99.93% | Val Loss: 0.2798 | Val Acc: 92.38% | f1: 0.92\n",
      "Epoch 28/50 | Train Loss: 0.0213 | Train Acc: 100.00% | Val Loss: 0.3128 | Val Acc: 90.34% | f1: 0.90\n",
      "Epoch 29/50 | Train Loss: 0.0204 | Train Acc: 100.00% | Val Loss: 0.2572 | Val Acc: 93.74% | f1: 0.94\n",
      "Epoch 30/50 | Train Loss: 0.0182 | Train Acc: 100.00% | Val Loss: 0.2540 | Val Acc: 93.74% | f1: 0.94\n",
      "Epoch 31/50 | Train Loss: 0.0190 | Train Acc: 100.00% | Val Loss: 0.2475 | Val Acc: 93.61% | f1: 0.94\n",
      "Epoch 32/50 | Train Loss: 0.0156 | Train Acc: 100.00% | Val Loss: 0.2492 | Val Acc: 93.88% | f1: 0.94\n",
      "Epoch 33/50 | Train Loss: 0.0135 | Train Acc: 100.00% | Val Loss: 0.2581 | Val Acc: 93.06% | f1: 0.93\n",
      "Epoch 34/50 | Train Loss: 0.0132 | Train Acc: 100.00% | Val Loss: 0.2503 | Val Acc: 93.33% | f1: 0.93\n",
      "Epoch 35/50 | Train Loss: 0.0133 | Train Acc: 100.00% | Val Loss: 0.2540 | Val Acc: 92.65% | f1: 0.93\n",
      "Epoch 36/50 | Train Loss: 0.0126 | Train Acc: 100.00% | Val Loss: 0.2552 | Val Acc: 93.47% | f1: 0.93\n",
      "Epoch 37/50 | Train Loss: 0.0101 | Train Acc: 100.00% | Val Loss: 0.2350 | Val Acc: 94.29% | f1: 0.94\n",
      "Epoch 38/50 | Train Loss: 0.0092 | Train Acc: 100.00% | Val Loss: 0.2421 | Val Acc: 94.29% | f1: 0.94\n",
      "Epoch 39/50 | Train Loss: 0.0088 | Train Acc: 100.00% | Val Loss: 0.2478 | Val Acc: 93.33% | f1: 0.93\n",
      "Epoch 40/50 | Train Loss: 0.0091 | Train Acc: 100.00% | Val Loss: 0.2610 | Val Acc: 92.79% | f1: 0.93\n",
      "Epoch 41/50 | Train Loss: 0.0089 | Train Acc: 100.00% | Val Loss: 0.2406 | Val Acc: 94.15% | f1: 0.94\n",
      "Epoch 42/50 | Train Loss: 0.0080 | Train Acc: 100.00% | Val Loss: 0.2455 | Val Acc: 93.88% | f1: 0.94\n",
      "Epoch 43/50 | Train Loss: 0.0077 | Train Acc: 100.00% | Val Loss: 0.2442 | Val Acc: 94.01% | f1: 0.94\n",
      "Epoch 44/50 | Train Loss: 0.0078 | Train Acc: 100.00% | Val Loss: 0.2462 | Val Acc: 93.20% | f1: 0.93\n",
      "Epoch 45/50 | Train Loss: 0.0089 | Train Acc: 100.00% | Val Loss: 0.2413 | Val Acc: 93.33% | f1: 0.93\n",
      "Epoch 46/50 | Train Loss: 0.0077 | Train Acc: 100.00% | Val Loss: 0.2530 | Val Acc: 93.33% | f1: 0.93\n",
      "Epoch 47/50 | Train Loss: 0.0064 | Train Acc: 100.00% | Val Loss: 0.2439 | Val Acc: 93.33% | f1: 0.93\n",
      "Epoch 48/50 | Train Loss: 0.0062 | Train Acc: 100.00% | Val Loss: 0.2457 | Val Acc: 93.06% | f1: 0.93\n",
      "Epoch 49/50 | Train Loss: 0.0056 | Train Acc: 100.00% | Val Loss: 0.2372 | Val Acc: 93.74% | f1: 0.94\n",
      "Epoch 50/50 | Train Loss: 0.0059 | Train Acc: 100.00% | Val Loss: 0.2528 | Val Acc: 92.79% | f1: 0.93\n"
     ]
    }
   ],
   "source": [
    "patience = 15 \n",
    "best_val_loss = float('inf')\n",
    "counter = 0  \n",
    "for epoch in range(EPOCHS):\n",
    "    avg_train_loss , train_accuracy = train_model(model, train_loader, criterion, optimizer)\n",
    "    avg_val_loss, val_accuracy ,f1 = evaluate_model(model, test_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}% | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | f1: {f1:.2f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0 \n",
    "        best_model = model.state_dict()  \n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}. Best val loss: {best_val_loss:.4f}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Micro F1 Score = 0.9279\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation and confusion matrix\n",
    "avg_val_loss, val_accuracy, f1 = evaluate_model(model, test_loader, criterion)\n",
    "print(f\"\\nFinal Micro F1 Score = {f1:.4f}\")\n",
    "label_names = [label for label, _ in sorted(train_data.label2idx.items(), key=lambda x: x[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dla3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
